{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Sentence Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name Supabase/gte-small. Creating a new one with MEAN pooling.\n",
      "/Users/ian/Code/school/techin510/sp24/lab7/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/ian/Code/school/techin510/sp24/lab7/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "# Here we are using this model from Supabase\n",
    "# https://huggingface.co/Supabase/gte-small\n",
    "# You can choose other embedding models from Hugging Face\n",
    "\n",
    "model = SentenceTransformer('Supabase/gte-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the embedding vector is 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.73097491e-01, -4.71553117e-01,  7.92207941e-02, -4.01285559e-01,\n",
       "       -7.17622265e-02, -1.38925672e-01,  3.35828155e-01,  2.56369591e-01,\n",
       "        2.25289520e-02,  3.36204357e-02, -2.89838374e-01, -6.86789393e-01,\n",
       "        4.86794055e-01,  2.61979014e-01, -9.76839513e-02, -2.52809286e-01,\n",
       "        2.07800120e-02, -1.89304184e-02, -3.48913461e-01,  8.48381668e-02,\n",
       "        1.59663633e-01, -2.12455124e-01, -4.23649013e-01, -1.02164638e+00,\n",
       "       -1.95467919e-02,  8.37713778e-01, -2.95442641e-01, -3.57815683e-01,\n",
       "       -1.60214499e-01, -1.29963005e+00,  1.48330862e-02, -2.96227276e-01,\n",
       "        6.15166247e-01, -2.47573540e-01, -1.04925102e-02,  9.18918923e-02,\n",
       "       -7.85714239e-02,  7.93742761e-02, -5.00591576e-01,  4.96645123e-01,\n",
       "        3.19249660e-01,  1.01673068e-03, -1.99556425e-01, -3.76710683e-01,\n",
       "       -2.29374900e-01, -7.59295404e-01, -4.37365860e-01, -2.10281089e-01,\n",
       "        1.39861107e-01, -3.03207129e-01, -1.64122246e-02, -3.51231426e-01,\n",
       "       -1.48414329e-01,  2.30724096e-01,  7.22426325e-02,  1.34261101e-01,\n",
       "        4.69965070e-01,  5.82754791e-01,  2.51384199e-01,  2.30032921e-01,\n",
       "        5.50779402e-02,  1.53820291e-01, -1.67925560e+00,  1.06108034e+00,\n",
       "        4.10480827e-01,  4.25345838e-01, -4.31208134e-01, -3.16961914e-01,\n",
       "        3.84941310e-01,  4.18763638e-01, -3.91376287e-01,  3.52411330e-01,\n",
       "       -1.76182777e-01,  7.63384521e-01,  2.69013882e-01, -4.79051858e-01,\n",
       "        1.75593451e-01, -9.78749990e-02, -5.38470857e-02,  1.76142722e-01,\n",
       "       -1.72431841e-01, -5.50874174e-01, -4.02812421e-01, -2.67538074e-02,\n",
       "       -2.02519938e-01, -1.28547087e-01,  1.58123985e-01,  5.02879880e-02,\n",
       "        6.03739381e-01, -1.48069292e-01, -3.50918829e-01, -1.25668660e-01,\n",
       "        5.48474714e-02,  3.95593643e-01, -2.81305254e-01, -2.18253508e-01,\n",
       "        8.18482518e-01,  2.28878722e-01, -8.07816923e-01,  2.39057207e+00,\n",
       "       -7.23874927e-01,  1.27721205e-01,  5.10972023e-01, -9.00969505e-02,\n",
       "        2.56729484e-01, -3.09500068e-01, -1.97674438e-01, -3.59676152e-01,\n",
       "       -2.33321235e-01, -4.37654644e-01,  1.12204831e-02, -3.90189104e-02,\n",
       "        5.22182882e-01, -5.92509270e-01,  2.00427577e-01,  2.10902646e-01,\n",
       "        1.30604887e-02,  3.10501277e-01,  1.88154384e-01,  2.66900063e-01,\n",
       "       -1.32748827e-01,  2.26465866e-01,  1.61466926e-01, -3.38359833e-01,\n",
       "        8.77118930e-02, -3.19922656e-01, -9.46280286e-02,  1.21279907e+00,\n",
       "       -1.50492802e-01,  6.63556233e-02,  7.47588933e-01, -3.02776068e-01,\n",
       "       -3.00123066e-01,  1.14895999e-01,  1.35791123e-01,  1.38370946e-01,\n",
       "        2.91894265e-02,  4.01588492e-02,  2.49607041e-01, -4.16370243e-01,\n",
       "       -5.89878261e-01, -7.80600429e-01,  1.04141071e-01, -8.28681946e-01,\n",
       "       -5.94447255e-02,  5.51878393e-01, -4.87513006e-01,  4.11349386e-01,\n",
       "       -4.96408008e-02,  1.35047004e-01, -4.62886304e-01,  2.21131653e-01,\n",
       "       -2.06025630e-01, -4.80446368e-01,  2.27054432e-01,  5.37203431e-01,\n",
       "        7.14850724e-01,  4.28999841e-01, -1.69408724e-01, -3.85715440e-02,\n",
       "       -7.81208515e-01, -4.65835720e-01, -3.16604346e-01,  2.84540713e-01,\n",
       "        2.46599197e-01, -1.09444153e+00, -9.20692012e-02,  2.32232898e-01,\n",
       "       -4.98940609e-02, -1.06713906e-01,  6.85922325e-01,  2.93636888e-01,\n",
       "       -4.00487214e-01,  1.04259931e-01,  6.60525858e-01,  1.77994221e-01,\n",
       "       -6.16076350e-01,  1.71653312e-02,  6.14981592e-01,  2.41238832e-01,\n",
       "        5.37724197e-01, -2.76388466e-01,  9.18902978e-02,  5.21830991e-02,\n",
       "        1.10705867e-01, -3.68918091e-01,  2.26359032e-02, -1.88334242e-01,\n",
       "        1.46673068e-01,  3.36590737e-01,  9.67472196e-02,  4.73908514e-01,\n",
       "       -3.60923380e-01, -3.49651098e-01, -2.29173526e-01, -2.81934321e-01,\n",
       "        2.30019301e-01, -1.79785833e-01,  3.17300335e-02, -3.68815511e-01,\n",
       "        2.13862747e-01, -3.03516518e-02, -8.03306103e-02,  4.04847801e-01,\n",
       "        7.06272945e-02, -1.12624221e-01, -2.37838164e-01, -1.77090764e-02,\n",
       "        5.70379138e-01,  2.31017232e-01, -3.70245248e-01, -3.02313328e-01,\n",
       "        8.36544335e-01, -5.25447369e-01, -6.61720634e-01, -3.01689416e-01,\n",
       "        1.14540182e-01,  5.78046262e-01,  4.29253727e-01,  4.24993098e-01,\n",
       "       -3.32890928e-01, -7.22259939e-01, -4.98783022e-01, -2.23008537e+00,\n",
       "        1.46413758e-01,  3.12166959e-01, -2.88334459e-01,  3.56475681e-01,\n",
       "       -6.74875915e-01,  1.19462848e-01, -1.11690454e-01,  3.18117999e-02,\n",
       "        9.39079225e-01,  8.34068239e-01, -1.50240496e-01, -3.18276882e-02,\n",
       "       -2.24551986e-04, -3.11028868e-01,  5.83415866e-01,  2.08439574e-01,\n",
       "       -1.23741515e-01, -1.85864851e-01,  1.30598798e-01, -1.36630079e-02,\n",
       "       -1.16155706e-01, -9.58686024e-02, -5.74641645e-01, -7.95620680e-02,\n",
       "       -3.79281580e-01,  2.16405916e+00,  4.78177488e-01,  5.61071694e-01,\n",
       "       -2.42512207e-02,  2.93545395e-01, -3.69016267e-02, -2.47656614e-01,\n",
       "       -1.50931418e+00,  3.36007923e-01,  5.00589788e-01,  4.52306211e-01,\n",
       "        8.56361389e-02, -2.53103971e-01, -3.86732519e-02, -7.26214796e-03,\n",
       "        3.94423902e-01,  1.38382599e-01, -3.99124295e-01, -2.55438656e-01,\n",
       "       -3.63638192e-01, -2.57787973e-01, -6.13501012e-01, -1.18779317e-01,\n",
       "        4.23876435e-01,  2.15329856e-01,  3.41630057e-02,  3.08157384e-01,\n",
       "        3.13883096e-01, -4.51870441e-01, -4.20885146e-01, -1.04983902e+00,\n",
       "        1.08295027e-02, -2.46983394e-01,  4.22886848e-01, -1.32794574e-01,\n",
       "       -6.99821591e-01,  1.00948386e-01, -2.04359159e-01,  5.93272746e-01,\n",
       "       -6.62202910e-02, -5.47729321e-02, -7.35988170e-02,  4.01309818e-01,\n",
       "       -3.10818881e-01,  7.85432756e-03,  9.26734507e-01, -3.50131691e-02,\n",
       "       -3.42216015e-01,  8.69672671e-02,  2.46448919e-01,  7.19989017e-02,\n",
       "        3.82374138e-01,  3.31507891e-01, -3.96711856e-01,  3.07661533e-01,\n",
       "       -9.93290991e-02,  1.61217619e-02,  2.00741082e-01,  2.92676121e-01,\n",
       "       -3.45340699e-01,  3.26453149e-01,  1.96727249e-03,  2.70185739e-01,\n",
       "       -1.42847970e-01, -1.39063612e-01,  5.18492870e-02, -1.60198584e-01,\n",
       "        7.63220266e-02,  2.73926258e-01,  1.69689789e-01, -2.55603766e+00,\n",
       "        4.43483770e-01, -2.10989788e-01,  2.48665020e-01, -1.27635822e-01,\n",
       "        4.11634237e-01,  3.13110083e-01, -9.13701579e-02, -4.31734234e-01,\n",
       "       -8.11506901e-03, -1.97630376e-02,  8.43477100e-02,  3.41818094e-01,\n",
       "        8.41155872e-02,  1.16359048e-01,  4.25500870e-01,  7.25745499e-01,\n",
       "       -4.08002913e-01,  1.82976648e-01,  9.96399745e-02,  3.34324151e-01,\n",
       "        3.88556659e-01,  2.12828684e+00, -2.59152859e-01,  7.26905167e-01,\n",
       "        2.24438280e-01, -6.12608232e-02,  3.15206736e-01,  4.35843080e-01,\n",
       "        2.50578076e-02, -6.21489286e-02, -7.85823613e-02,  7.87079036e-01,\n",
       "       -4.09282416e-01,  2.04810113e-01,  2.42820099e-01, -8.95206779e-02,\n",
       "        2.60687530e-01,  3.79182175e-02, -8.96804873e-03, -3.59439671e-01,\n",
       "        8.83229300e-02, -6.41342461e-01, -4.02516127e-01,  6.63487971e-01,\n",
       "       -1.83429584e-01, -5.54243661e-02, -7.41993248e-01,  3.12549263e-01,\n",
       "        2.49037609e-01, -3.09501261e-01, -7.60171339e-02, -3.30602080e-01,\n",
       "        8.49305168e-02,  3.36896688e-01,  5.73989809e-01,  1.73160006e-04,\n",
       "       -5.25865220e-02, -2.33384341e-01, -2.06953004e-01,  2.58361071e-01,\n",
       "       -6.85355842e-01,  3.86066765e-01,  5.49751520e-01, -3.13830301e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's use the model and convert the sentence into embeddings\n",
    "\"\"\"\n",
    "\n",
    "embeddings = model.encode(\"I like Python programing\")\n",
    "print(f\"the length of the embedding vector is {len(embeddings)}\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8980]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can use embeddings to find the similarity between two sentences.\n",
    "Here we are using cosine similarity to find the similarity between two sentences.\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors.\n",
    "\n",
    "Maximum similarity is 1 and minimum similarity is -1.\n",
    "\"\"\"\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('This recent movie is so good')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7360]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not so similar sentences\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('I like Python programming')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exactly same sentences\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('The new movie is awesome')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.8200\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.7016\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9697\n"
     ]
    }
   ],
   "source": [
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "    \"The new movie is awesome\",\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"The dog plays in the garden\",\n",
    "    \"A woman watches TV\",\n",
    "    \"The new movie is so great\",\n",
    "]\n",
    "\n",
    "# Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(\n",
    "        sentences1[i], sentences2[i], cosine_scores[i][i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man ate food. (Score: 0.8466)\n",
      "A man is eating a piece of bread. (Score: 0.8153)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A common use case of sentence embeddings is semantic search.\n",
    "\n",
    "Here we embed a list of documents and a query. Then we find the most similar documents to the query.\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "docs = [\n",
    "    \"A man ate food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "\n",
    "docs_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "\"\"\"\n",
    "Try different queries and see the results\n",
    "\"\"\"\n",
    "query = \"I am hungry\"\n",
    "#query = \"Tell me about music\"\n",
    "#query = \"What is moving?\"\n",
    "\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# You can set the number of results you want by changing the top_k parameter\n",
    "hits = semantic_search(query_embedding, docs_embeddings, top_k=2)\n",
    "\n",
    "for hit in hits[0]:\n",
    "    print(docs[hit['corpus_id']], \"(Score: %.4f)\" % hit['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look into ReRanking for better results\n",
    "https://sbert.net/examples/applications/retrieve_rerank/README.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "Why use tokens?\n",
    "\n",
    "> By breaking words into smaller parts (tokens), LLMs can better handle new or unusual words by understanding their building blocks. It also helps the model grasp the nuances of language, such as different word forms and contextual meanings.\n",
    "\n",
    "[source](https://kelvin.legal/understanding-large-language-models-words-versus-tokens/#:~:text=By%20breaking%20words%20into%20smaller,word%20forms%20and%20contextual%20meanings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split by whitespace: 20\n",
      "encoded sentence: 22\n",
      "tokens: [b'If', b' we', b' split', b' a', b' text', b' by', b' number', b' of', b' characters', b',', b' it', b' is', b' not', b' obvious', b' how', b' many', b' tokens', b' these', b' chunks', b' will', b' be', b'.']\n",
      "22\n",
      "reconstructed words: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If we split a text by number of characters, it is not obvious how many tokens these chunks will be.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "sent = \"If we split a text by number of characters, it is not obvious how many tokens these chunks will be.\"\n",
    "\n",
    "print(\"Split by whitespace: %s\"%len(sent.split()))\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoded = enc.encode(sent)\n",
    "\n",
    "print(\"encoded sentence: %s\"%len(encoded))\n",
    "\n",
    "tokens = [enc.decode_single_token_bytes(x) for x in encoded]\n",
    "print(\"tokens: %s\"%tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "\n",
    "decoded = enc.decode(encoded)\n",
    "print(\"reconstructed words: %s\"%len(decoded.split()))\n",
    "decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def split_large_text(large_text, max_tokens):\n",
    "    \"\"\"Convenience function to split a large text into chunks of max_tokens tokens.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokenized_text = enc.encode(large_text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for token in tokenized_text:\n",
    "        current_chunk.append(token)\n",
    "        current_length += 1\n",
    "\n",
    "        if current_length >= max_tokens:\n",
    "            chunks.append(enc.decode(current_chunk).rstrip(' .,;'))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(enc.decode(current_chunk).rstrip(' .,;'))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If we split a text by number of characters',\n",
       " ' it is not obvious how many tokens these chunks will',\n",
       " ' be.\\nAnd at the same time if we want',\n",
       " ' to split a text into bigger possible chunks and keep',\n",
       " ' these chunks under certain LLM tokens limit, we',\n",
       " ' cannot operate by number of characters']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\"If we split a text by number of characters, it is not obvious how many tokens these chunks will be.\n",
    "And at the same time if we want to split a text into bigger possible chunks and keep these chunks under certain LLM tokens limit, we cannot operate by number of characters.\"\"\"\n",
    "split_large_text(doc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the following context, then answer the question based on the context only\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = input(\"Enter the question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me about the new iPad'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = semantic_search(question, docs, top_k=2)\n",
    "\n",
    "[\"features are xxxxxx\", \"difference from preivous perverions xxxxx\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the following context, then answer the question based on the context only\n",
    "\n",
    "Context:\n",
    "\n",
    "Apple released the iPad with xxxxxx\n",
    "\n",
    "It is difrfernt from preisou....\n",
    "\n",
    "\n",
    "Question:\n",
    "\n",
    "Tell me about the new iPad\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to read PDF in Python\n",
    "\n",
    "Here we are only reading the text, for images, tables, and formulas, we need to use OCR based solutions like nougat from Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Obtaining dependency information for pymupdf from https://files.pythonhosted.org/packages/10/be/c1a8afad3a3c1a10023548dc037c6b86b5ab8c234b6b8bc53a89c8d26051/PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from pymupdf)\n",
      "  Obtaining dependency information for PyMuPDFb==1.24.3 from https://files.pythonhosted.org/packages/7e/4a/27e4e2ce8f5d0ed1d1b2a1f7807f6158db1e8e547a7bf76ac462a800a4b4/PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"icis24a-sub2705-i8.pdf\") # open a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for page in doc: # iterate the document pages\n",
    "    text += page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative AI and Evaluations of Early-Stage Innovations\\nGenerative AI and Evaluations of Early-Stage\\nInnovations\\nShort Paper\\nIntroduction\\nThe rapid advancement of AI is creating unprecedented opportunities for generating novel ideas\\ncost-effectively and at scale across a range of innovative contexts, such as crowdsourcing (Boussioux et al.,\\n2023), consumer products (Girotra et al., 2023), knowledge work (Dell’Acqua et al., 2023), and creative\\nwriting (Doshi & Hausman, 2023). The unparalleled growth in ideas necessitates alternative screening\\nmethods. Large Language Models (LLMs) offer a promising approach to assist experts in filtering and\\nprioritizing ideas with the potential for high impact. This research investigates how the use and design of\\nAI-driven evaluation tools influence experts’ ability to discern and make pivotal decisions on advancing or\\nrejecting novel ideas.\\nRecent studies have investigated the potential of human-AI collaboration in decision-making processes,\\nparticularly in evaluative and diagnostic tasks (Lebovitz et al. 2022), drug discovery (Lou & Wu, 2021),\\nand hiring (Li et al. 2020). However, to date, there have been few opportunities to explore the potential\\nfor human-AI collaboration for the screening and selection of early-stage ideas. Unlike tasks with clear\\nrules, patterns, objectives, and observable ground truth labels such as medical diagnosis and treatment\\nselection (Agarwal et al. 2023; Lebovitz et al. 2022), for which AI models can be trained on past data to\\nmake predictions, novel ideas are unique and depart from the existing knowledge frontier (Amabile, 1983;\\nBoudreau et al., 2016) removing the possibility to validate them against reliable full-proof outcomes.\\nHence, it is unclear whether AI can effectively screen novel ideas–for which there is often uncertainty\\nregarding their potential, which can be challenging to infer over time, even if the idea is eventually\\nexecuted (Boudreau et al., 2016).\\nFurthermore, innovation challenges, such as venture pitch competitions, accelerators, grant funding\\nopportunities, and crowdsourcing contests, often attract many more ideas that exceed the resources the\\nchallenge organizers can provide (Bell et al., 2023). Consequently, the first stage of many innovation\\nchallenges is to screen out low-quality ideas to allow a smaller subset of viable ideas to advance to the next\\nevaluation stages. As the number of potential ideas expands, this process can be cognitively burdening for\\nchallenge organizers–and often results in using heuristics or mental shortcuts to simplify decision-making\\n(Kahneman and Tversky, 1979). Although heuristics can expedite screening, prior literature has shown\\nthat they can lead to the use of intuition or “gut feel” over scientific analyses (Huang & Pearce, 2015) that\\npotentially prioritize status and familiarity over ideas from underrepresented groups, such as women and\\nminorities (Burton, Sorensen, and Beckman 2002). Such suboptimal decision-making may be further\\namplified in global competitions that attract large numbers of participants worldwide.\\nOur research aims to address this critical gap by exploring how Generative AI can assist in human\\ndecision-making in evaluating early-stage innovations. We partner with [Anonymous], a global\\nentrepreneurship platform focused on social impact. We design a Generative AI decision-support screener\\nthat provides recommendations of whether to pass or fail each idea, along with the criterion of failure and\\nthe rationale for its decision. We layer a field experiment onto [Anonymous]’s 2024 Global Health Equity\\nchallenge by randomizing whether their internal staff are exposed to the AI’s recommendations during\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n1\\nGenerative AI and Evaluations of Early-Stage Innovations\\ntheir screening process. To complement the internal staff decisions, we also experimentally test the AI\\nrecommendations\\nwith\\nfinancial\\nsponsors\\nand\\nexperienced\\nprofessionals\\nfrom venture capital,\\nentrepreneurial, and corporate investing backgrounds.\\nThe study compares three experimental conditions using a within-subjects design: a control condition\\n(human-only evaluation), treatment 1 (human-AI collaboration with AI recommendations), and treatment\\n2 (human-AI collaboration with AI recommendations and rationale explaining its choices). Each\\nparticipant is randomly assigned 30 submissions in two experimental conditions in a randomized\\nsequence, thus allowing for a comprehensive comparison of the different evaluation methodologies. We\\ncollect data on several key dependent variables, including time to decision, stringency of decisions,\\nalignment with AI decisions, and user behaviors. Additionally, we conduct post-evaluation surveys and\\nin-depth interviews to gather insights into participant perceptions and experiences.\\nWe report on the pilot experiment’s results with 18 internal Staff screeners from [Anonymous]. Our initial\\nfindings indicate that while all screeners use AI’s recommendations to assist in their decision-making,\\nthey\\nmay\\nuse\\nAI’s\\ninsights\\nstrategically:\\nchoosing to validate their decisions when the AI’s\\nrecommendations agree with their own and scrutinize AI’s recommendations when they disagree.\\nContrary to expectations, we did not find that receiving AI decision support led to more efficient screening\\ntimes per solution. These results will be updated in June 2024 to include an enlarged sample of\\nprofessional judges with corporate executive, venture capital, and entrepreneurial backgrounds.\\nThis study contributes to our understanding of AI-assisted decision-making, particularly in evaluating\\nearly-stage innovations. Our findings illustrate how AI can effectively assist humans in managing the\\nincreased volume of submissions–an essential tool in the era of Generative AI, where ideas are plentiful\\nbut discerning quality remains challenging. Despite the challenge organizers setting a target pass rate of\\n50 percent for the initial screening phase, our findings indicate that human screeners were inclined to\\napprove more submissions, likely to avoid overlooking viable ideas (false negatives) rather than excluding\\npotential failures (false positives). The AI support tool proved effective in helping screeners more\\nrigorously eliminate less promising ideas during this initial phase. In this respect, human judgment\\nproved crucial in making the final decisions to pass or reject submissions.\\nBy elucidating the conditions under which AI assistance is most beneficial, our research provides a\\nfoundation for developing more effective, efficient, and human-centered AI systems that leverage the\\nsynergies between human intuition and AI processing capabilities. The insights gained from this study\\nmay have significant implications for various domains where evaluative processes are crucial, such as\\nacademic research, grant funding, and competitive selection processes.\\nTheoretical Foundation\\nHuman-AI Collaboration in the Screening of Early-Stage Innovations\\nIntegrating AI into the screening process of early-stage innovations offers the potential to overcome\\nlimited attention spans and biases in mental models innate to human evaluators, thereby leading to a\\nbetter selection of high-impact innovations for further development. Whereas most uses of AI in\\ninnovation tend to focus on the idea generation stage (e.g., Boussioux et al. 2023; Girotra et al. 2023), few\\nstudies have investigated how AI can be integrated into the screening stage of evaluations. The increasing\\nvolume and complexity of ideas–potentially enhanced by Generative AI–necessitates the investigation of\\nalternative approaches to traditional human-only processes.\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n2\\nGenerative AI and Evaluations of Early-Stage Innovations\\nWhen considering the use of AI in screening early-stage innovations, there are compelling reasons both\\nfor and against relying on AI recommendations. On one hand, AI enhances efficiency and speed, allowing\\nfor the rapid processing of large datasets, which can expedite decision-making processes (Lou and Wu,\\n2021). AI can support data-driven decisions that can minimize personal biases and rely on its strong\\npattern recognition capabilities to identify promising opportunities that might not be immediately\\nobvious. AI systems can handle a much larger volume of submissions than human screeners, ensuring\\nconsistency in assessment regardless of the scale.\\nOn the other hand, there are notable drawbacks to consider. AI systems may lack a nuanced\\nunderstanding of complex contexts, often crucial in emerging technologies or markets where societal\\ntrends and ethical considerations play significant roles. The reliance on historical data might also limit\\nAI’s effectiveness, particularly in fields where past trends might not accurately predict future outcomes.\\nIssues of explainability can lead to mistrust in AI recommendations (Lebovitz et al. 2022), especially in\\nscenarios where stakeholders require clear justification for decisions. Moreover, inherent biases in\\ntraining data can perpetuate inequalities, making AI less reliable (Li et al., 2020). Finally, the invaluable\\nhuman expertise and intuition, which are crucial in understanding multifaceted and evolving situations,\\ncannot be fully replicated by AI, posing a risk of over-dependence that might diminish human analytical\\ncapacities over time (Swaroop et al. 2024).\\nGiven AI’s advantages and limitations in screening early-stage innovations, it becomes evident that a\\ncollaborative approach between human experts and AI systems is essential to augment decision-making.\\nExplainability of AI-Assisted Decision-Making\\nExplainability plays a critical role in this collaborative framework (Wang et al. 2021). It refers to the\\nability of AI systems to describe or rationalize their processes and decisions in a way that is\\nunderstandable to humans (Vasconcelos et al. 2023). Prior work has suggested that humans can become\\noverreliant on AI’s recommendations by forgoing agency and accountability to the AI when making final\\ndecisions (Vasconcelos et al., 2023). It is a particular concern in high-stakes domains, such as the\\nscreening of early-stage innovations, and can run the risk of reinforcing machine bias. When AI systems\\nprovide clear, comprehensible explanations for their recommendations, human partners can more\\neffectively assess the validity and relevance of these suggestions (Ma et al. 2024). Explainability can help\\nbridge the gap between AI’s data-driven analysis and human contextual insights, allowing for a more\\ninformed and nuanced evaluation of potential innovations.\\nFurthermore, explainability may aid human screeners in rationalizing their own decisions, particularly\\nwhen the design of AI systems reflects the interests of the organizations that deploy them over the private\\ninterests of the screeners. In particular, AI screeners may prioritize the broader objectives of\\norganizations, which can lead to recommendations that are either more lenient or stricter than a human\\nscreener’s own judgments. In this respect, human screeners may use AI’s recommendations differently\\ndepending on whether their own beliefs prioritize avoiding false negatives (unjust rejections) or false\\npositives (incorrect acceptance). Hence, we might expect human screeners to use AI’s recommendations\\nstrategically, to validate their underlying beliefs when AI’s recommendations are consistent with their\\nown, as opposed to scrutinizing the AI’s decisions when they depart from their own decisions.\\nMethodology\\nExperimental Design\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n3\\nGenerative AI and Evaluations of Early-Stage Innovations\\nWe conduct within-subjects experiments with three conditions: a control condition (human-only\\nevaluation), treatment 1 (human-AI collaboration with AI recommendations), and treatment 2 (human-AI\\ncollaboration with AI recommendations and rationale). Participants evaluated a randomized subset of\\nsolutions from [Anonymized]’s 2024 Global Health Equity challenge. Each participant evaluated\\nsubmissions in two conditions in a randomized order, allowing for a comprehensive and balanced\\ncomparison of the different evaluation methodologies.\\nDesigning the AI Screening Recommendations\\n[Anonymous] platform uses a systematic process to screen submissions to their global entrepreneurship\\nchallenge. The screening process assesses a submission using five criteria: Criterion 1 - Is the solution\\napplication complete, appropriate, and intelligible?; Criterion 2 - Is the solution at least in Prototype\\nstage?; Criterion 3 - Does the solution address the Challenge question?; Criterion 4 - Is the solution\\npowered by technology?; Criterion 5 - The quality of the solution is good enough that an external reviewer\\nshould take the time to read and score it\\nA submission is screened out if it fails to meet one or more criteria. In April 2024, we used OpenAI’s\\nGPT-4, a state-of-the-art Large Language Model (LLM), to develop the AI screener model to derive a\\npass/fail decision for each submission and elicit explanations and rationales. By carefully crafting\\nprompts that encourage the LLM to provide clear and concise justifications for its evaluations, users can\\ngain valuable insights into the factors influencing the model’s decisions, facilitating a more informed and\\ncollaborative screening process (Girotra et al. 2023).\\nTo calibrate and refine the decision produced by the LLM for screening, we prompted the models using\\nChain of Thought (CoT) mechanisms and leveraged decisions from [Anonymized]’s past screening\\ndecisions to suggest “few-shot” examples of failure and success for each of the five criteria. This process\\nled to generating AI recommendations, accompanied by rationale (limited to 250 words), to pass or fail a\\nsubmission on each criterion. To determine the final set of passed and failed submissions, we prompted\\nGPT-4 to develop a confidence score (expressed as a percentage) for its decision on each criterion. Using\\nthe challenge organizers’ objective of screening out roughly 50 percent of the initial submissions, we set a\\nconfidence score threshold of 75%. This resulted in 22 of 48 (46%) submissions with ‘pass’ AI\\nrecommendations, consistent with the organization’s passing threshold of 50%.\\nPilot Study Procedures\\nWe conducted a pilot study to test the feasibility of the experiment and show initial results. Participants\\nwere recruited from among staff members at [Anonymized] and asked whether they would be willing to\\nparticipate in a 90-minute experiment. A total of 18 participants took part in and completed the pilot\\nstudy. Data collection took place on April 30, 2024. A follow-up study is scheduled in late May 2024, with\\ndata collection to be collected by June 30, 2024.\\nSubmission Evaluation\\nA short introduction on Qualtrics explains the rules of the evaluation process and motivates serious\\nengagement. Participants are told that they will evaluate a series of submissions based on the five criteria\\nused by [Anonymized]’s screeners in previous years.\\nWe have three experimental conditions. In the control condition, participants evaluate the submissions\\nwithout assistance from the generative AI tool, replicating the current evaluation process. In treatment 1,\\nparticipants can see the AI’s pass/fail recommendations on each criterion to aid their screening decisions.\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n4\\nGenerative AI and Evaluations of Early-Stage Innovations\\nIn treatment 2, participants receive both the AI’s recommendations and the rationale behind those\\nrecommendations, providing increased transparency into the AI’s decision-making process. The\\nparticipants are aware the decision support tool is based on AI. We randomly selected 48 out of 529\\nsubmissions from [Anonymized]’s 2024 Global Health Equity challenge. Each participant evaluated 30\\nsubmissions in two experimental conditions (15 each) in one of the six possible randomized sequences to\\nensure a balanced design.\\nThe evaluation process is facilitated through a specially designed interface, as shown in Figure 1, hosted\\non the [Anonymized] cloud computing platform. Participants access the interface via a URL link provided\\nthrough Qualtrics. The interface presents submission details used to make a screening decision (pass or\\nfail) for each submission based on the five predefined criteria.\\nFigure 1. Web interface (treatment 2) used to evaluate a solution. On the left, the\\nrecommendations obtained from GPT4, with the summarized rationale. On the right, the\\nanswers submitted to the challenge by a global human solver.\\nMeasurement of Variables\\nThroughout the experiment, we collect data on several key dependent variables to assess the outcomes of\\nthe different evaluation conditions: 1. Time to decision: Participants’ time to complete each evaluation. 2.\\nStringency of decisions: The proportion of pass (vs. fail) decisions made by participants. 3. Alignment\\nwith AI decisions: The degree to which participants align with the AI’s evaluation within the treatment\\ngroups. 4. User behaviors: The mouse movements and time spent on specific application parts using the\\nPostHog system will be used to determine how AI recommendations alter attention allocation and time\\nmanagement relative to the human-only process.\\nAfter the evaluations, participants completed a short survey to gather additional data on their perceptions,\\nexperiences, and overall satisfaction with the evaluation process.\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n5\\nGenerative AI and Evaluations of Early-Stage Innovations\\nData Analysis and Initial Results\\nBelow we report initial results from our pilot study with [Anonymized]’s expert screeners. This section\\nwill be updated in June 2024 to include the full experimental results encompassing a larger set of\\nprofessional screeners (e.g., VCs, executives, entrepreneurs, MBAs).\\nTime to Decision. First, we investigated the time spent evaluating each solution, comparing the\\ndifferences across the three conditions. Overall, the screeners spent, on average, 1.98 minutes per solution\\n(s.d. = 0.50 minutes). Figure 2a shows no meaningful differences in time spent across the experimental\\nconditions.\\nFigure 2a (left). Average Time Spent Evaluating Solutions By\\nExperimental Condition (Seconds) Figure 2b (right). Proportion of\\nDecisions Assigned a Pass Rating By Experimental Condition\\nStringency of Decisions (% Pass): Next, we compare the stringency of different experimental conditions\\nby investigating the proportion of evaluation passed across the three conditions (see Figure 2b). Whereas\\n70.6% of screener-solution pairs were assigned a pass rating in the control condition, 65.5% passed in\\nTreatment 1, and 58.3% passed in Treatment 2. A t-test indicates that the difference between the control\\ncondition and Treatment 2 is statistically significant (p = 0.015).\\nAlignment with AI’s Recommendation: Third, we investigate how the screening decisions aligned with\\nAI’s recommendations. Whereas 55.6% of the control condition’s decisions were aligned with AI’s\\nrecommendations, we observe that 71.7% of Treatment 1 (p = 0.001) and 66.1% of Treatment 2’s decisions\\n(p=0.040) were aligned with AI’s. This suggests that AI’s recommendations strongly influenced the\\nscreeners’ decisions, but there was no significant difference between the two treatment conditions.\\nFigure 3b investigates differences in alignment according to the AI’s recommendation to pass or fail a\\nsolution. We observe that AI’s recommendations were overall more stringent (i.e., more failed solutions)\\nthan the human’s recommendations (i.e., the control condition). Whereas there is no difference in\\nalignment across the experimental conditions for the passing decisions, we find a significant difference\\namong the failed solutions. In particular, 37% in the control condition failed to pass the screening criteria,\\ncompared to 56% in Treatment 1 (p=0.009) and 56% in Treatment 2 (p=0.005). This suggests that\\nscreeners may have used AI’s recommendations differently based on pass/fail suggestions.\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n6\\nGenerative AI and Evaluations of Early-Stage Innovations\\nFigure 3a (left). Proportion of Screening Decisions Aligned with AI’s\\nRecommendations Figure 3b (right). Proportion of Screening Decisions\\nAligned with AI’s Recommendations by Decision Type (Pass vs. Fail)\\nDiscussion, Contribution, and Future Research\\nThis study explores the potential of human-AI collaboration in improving the screening stage of the\\nevaluation process of early-stage innovations, focusing on efficiency, decision alignment, and overall\\ndecision quality. By partnering with [Anonymized] and designing a state-of-the-art generative AI\\nevaluation system, we aim to provide valuable insights into the optimal model of human-AI collaboration\\nin the context of evaluating innovative submissions.\\nThe pilot study results suggest that the experimental design is largely appropriate and offers valuable\\ninsights for possible alterations in the follow-up study. Additionally, we will gather qualitative data\\nthrough a post-evaluation survey and in-depth interviews to better understand participants’ experiences\\nand perceptions. This mixed-methods approach will provide a more holistic view of the impact of\\nhuman-AI collaboration on the screening process.\\nOur research contributes to the growing field of AI-assisted decision-making by investigating the role of\\nAI autonomy and explainability in evaluating early-stage innovations. By leveraging the strengths of\\nhuman intuition and AI’s processing capabilities, we aim to develop more effective, efficient, and\\nhuman-centered AI systems that optimize the synergies between humans and AI. Understanding how\\nindividuals attribute responsibility to AI systems in success and failure cases can help avoid unintended\\nconsequences and design more effective human-AI collaboration models. Our findings will provide\\nvaluable insights for researchers, practitioners, and policymakers, guiding the development and\\nimplementation of AI-assisted evaluation systems that significantly improve decision-making processes\\nacross various domains.\\nFrom a managerial perspective, the findings of our study will be relevant when designing collaboration\\nroles in an organizational context. Understanding the factors that influence the adoption and acceptance\\nof AI-assisted evaluation systems will allow for the development of tailored approaches to human-AI\\ncollaboration, considering the unique characteristics of both the users and the tasks at hand. Our\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n7\\nGenerative AI and Evaluations of Early-Stage Innovations\\npreliminary findings suggest that AI systems can better align the organization’s objectives with those of\\ntheir employees.\\nReferences\\nAgarwal, N., Moehring, A., Rajpurkar, P., & Salz, T. (2023). Combining Human Expertise with Artificial\\nIntelligence: Experimental Evidence from Radiology. MIT Economics.\\nAmabile TM (1983) “The social psychology of creativity: A componential conceptualization.” J. Pers. Soc.\\nPsychol. 45(2):357\\nBell, J. J., Pescher, C., Tellis, G. J., & Füller, J. (2024). Can AI help in ideation? A theory-based model for\\nidea screening in crowdsourcing contests. Marketing Science, 43(1), 54-72.\\nBoudreau KJ, Guinan EC, Lakhani KR, Riedl C (2016) “Looking across and looking beyond the knowledge\\nfrontier:\\nIntellectual\\ndistance,\\nnovelty,\\nand\\nresource\\nallocation\\nin\\nscience.”\\nManag.\\nSci.\\n62(10):2765–2783.\\nBoussioux, Leonard and N. Lane, Jacqueline and Zhang, Miaomiao and Jacimovic, Vladimir and Lakhani,\\nKarim R., “Generative AI and Creative Problem Solving” (March 2024). Harvard Business School\\nTechnology & Operations Mgt. Unit Working Paper No. 24-005, Available at SSRN\\nBurton, M. D., Sørensen, J. B., Beckman, C. M. 2002. “Coming from good stock: Career histories and new\\nventure formation,” in Research in the Sociology of Organizations, M. Lounsbury and M. J. Ventresca\\n(eds.), Vol. 19, pp. 229-262.\\nChen, V., Liao, Q. V., Wortman Vaughan, J., & Bansal, G. (2023). “Understanding the role of human\\nintuition on reliance in human-AI decision-making with explanations.” Proceedings of the ACM on\\nHuman-computer Interaction, 7(CSCW2), 1-32.\\nCriscuolo, P., Dahlander, L., Grohsjean, T., & Salter, A. (2017). Evaluating novelty: the role of panels in\\nthe\\nselection\\nof\\nR&D\\nprojects.\\nAcademy\\nof\\nManagement\\nJournal,\\n60(2),\\n433-460.\\nhttps://doi.org/10.5465/amj.2014.0861\\nDell’Acqua F, McFowland E, Mollick ER, Lifshitz-Assaf H, Kellogg K, Rajendran S, Krayer L, Candelon F,\\nLakhani KR (2023) Navigating the jagged technological frontier: Field experimental evidence of the\\neffects of AI on knowledge worker productivity and quality. Harv. Bus. Sch. Technol. Oper. Mgt Unit\\nWork. Pap. (24–013).\\nDoshi AR, Hauser O (2023) Generative artificial intelligence enhances creativity. Available SSRN.\\nGirotra K, Meincke L, Terwiesch C, Ulrich KT (2023) Ideas are dimes a dozen: Large language models for\\nidea generation in innovation. Available SSRN 4526071.\\nHuang, L., & Pearce, J. L. (2015). Managing the Unknowable: The Effectiveness of Early-stage Investor\\nGut Feel in Entrepreneurial Investment Decisions. Administrative Science Quarterly, 60(4), 634-670.\\nKahneman, D., Tversky, A. 1979. “Prospect Theory: An Analysis of Decision under Risk,” in Econometrica,\\nVol. 47, No. 2, pp. 263-291.\\nLebovitz S, Lifshitz-Assaf H, Levina N (2022) To engage or not to engage with AI for critical judgments:\\nHow professionals deal with opacity when using AI for medical diagnosis. Organ. Sci. 33(1):126–148.\\nLi D, Raymond LR, Bergman P (2020) Hiring as exploration (National Bureau of Economic Research).\\nLou B, Wu L (2021) AI on drugs: Can artificial intelligence accelerate drug development? Evidence from a\\nlarge-scale examination of bio-pharma firms. Evid. Large-Scale Exam. Bio-Pharma FirmsMarch 15\\n2021 MISQ Forthcom\\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., Ma, X. 2024. “Towards Human-AI Deliberation:\\nDesign and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making,”\\narXiv:2403.16812\\nSwaroop, S., Buçinca, Z., Gajos, K. Z., & Doshi-Velez, F. (2024, March). Accuracy-Time Tradeoffs in\\nAI-Assisted Decision Making under Time Pressure. In Proceedings of the 29th International\\nConference on Intelligent User Interfaces (pp. 138-154).\\nVasconcelos, H., Jörke, M., Grunde-McLaughlin, M., Gerstenberg, T., Bernstein, M. S., & Krishna, R.\\n(2023). Explanations can reduce overreliance on ai systems during decision-making. Proceedings of\\nthe ACM on Human-Computer Interaction, 7(CSCW1), 1-38.\\nWang, X., Yin, M. 2021. “Are Explanations Helpful? A Comparative Study of the Effects of Explanations in\\nAI-Assisted Decision-Making,” in Proceedings of the 26th International Conference on Intelligent\\nUser Interfaces, College Station, TX, USA: Association for Computing Machinery, pp. 318-328.\\nForty-Fifth International Conference on Information Systems, Bangkok, Thailand 2024\\n8\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
