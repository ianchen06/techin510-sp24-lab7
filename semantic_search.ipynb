{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Sentence Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name Supabase/gte-small. Creating a new one with MEAN pooling.\n",
      "/Users/ian/Code/school/techin510/sp24/lab7/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/ian/Code/school/techin510/sp24/lab7/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "# Here we are using this model from Supabase\n",
    "# https://huggingface.co/Supabase/gte-small\n",
    "# You can choose other embedding models from Hugging Face\n",
    "\n",
    "model = SentenceTransformer('Supabase/gte-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the embedding vector is 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.73097491e-01, -4.71553117e-01,  7.92207941e-02, -4.01285559e-01,\n",
       "       -7.17622265e-02, -1.38925672e-01,  3.35828155e-01,  2.56369591e-01,\n",
       "        2.25289520e-02,  3.36204357e-02, -2.89838374e-01, -6.86789393e-01,\n",
       "        4.86794055e-01,  2.61979014e-01, -9.76839513e-02, -2.52809286e-01,\n",
       "        2.07800120e-02, -1.89304184e-02, -3.48913461e-01,  8.48381668e-02,\n",
       "        1.59663633e-01, -2.12455124e-01, -4.23649013e-01, -1.02164638e+00,\n",
       "       -1.95467919e-02,  8.37713778e-01, -2.95442641e-01, -3.57815683e-01,\n",
       "       -1.60214499e-01, -1.29963005e+00,  1.48330862e-02, -2.96227276e-01,\n",
       "        6.15166247e-01, -2.47573540e-01, -1.04925102e-02,  9.18918923e-02,\n",
       "       -7.85714239e-02,  7.93742761e-02, -5.00591576e-01,  4.96645123e-01,\n",
       "        3.19249660e-01,  1.01673068e-03, -1.99556425e-01, -3.76710683e-01,\n",
       "       -2.29374900e-01, -7.59295404e-01, -4.37365860e-01, -2.10281089e-01,\n",
       "        1.39861107e-01, -3.03207129e-01, -1.64122246e-02, -3.51231426e-01,\n",
       "       -1.48414329e-01,  2.30724096e-01,  7.22426325e-02,  1.34261101e-01,\n",
       "        4.69965070e-01,  5.82754791e-01,  2.51384199e-01,  2.30032921e-01,\n",
       "        5.50779402e-02,  1.53820291e-01, -1.67925560e+00,  1.06108034e+00,\n",
       "        4.10480827e-01,  4.25345838e-01, -4.31208134e-01, -3.16961914e-01,\n",
       "        3.84941310e-01,  4.18763638e-01, -3.91376287e-01,  3.52411330e-01,\n",
       "       -1.76182777e-01,  7.63384521e-01,  2.69013882e-01, -4.79051858e-01,\n",
       "        1.75593451e-01, -9.78749990e-02, -5.38470857e-02,  1.76142722e-01,\n",
       "       -1.72431841e-01, -5.50874174e-01, -4.02812421e-01, -2.67538074e-02,\n",
       "       -2.02519938e-01, -1.28547087e-01,  1.58123985e-01,  5.02879880e-02,\n",
       "        6.03739381e-01, -1.48069292e-01, -3.50918829e-01, -1.25668660e-01,\n",
       "        5.48474714e-02,  3.95593643e-01, -2.81305254e-01, -2.18253508e-01,\n",
       "        8.18482518e-01,  2.28878722e-01, -8.07816923e-01,  2.39057207e+00,\n",
       "       -7.23874927e-01,  1.27721205e-01,  5.10972023e-01, -9.00969505e-02,\n",
       "        2.56729484e-01, -3.09500068e-01, -1.97674438e-01, -3.59676152e-01,\n",
       "       -2.33321235e-01, -4.37654644e-01,  1.12204831e-02, -3.90189104e-02,\n",
       "        5.22182882e-01, -5.92509270e-01,  2.00427577e-01,  2.10902646e-01,\n",
       "        1.30604887e-02,  3.10501277e-01,  1.88154384e-01,  2.66900063e-01,\n",
       "       -1.32748827e-01,  2.26465866e-01,  1.61466926e-01, -3.38359833e-01,\n",
       "        8.77118930e-02, -3.19922656e-01, -9.46280286e-02,  1.21279907e+00,\n",
       "       -1.50492802e-01,  6.63556233e-02,  7.47588933e-01, -3.02776068e-01,\n",
       "       -3.00123066e-01,  1.14895999e-01,  1.35791123e-01,  1.38370946e-01,\n",
       "        2.91894265e-02,  4.01588492e-02,  2.49607041e-01, -4.16370243e-01,\n",
       "       -5.89878261e-01, -7.80600429e-01,  1.04141071e-01, -8.28681946e-01,\n",
       "       -5.94447255e-02,  5.51878393e-01, -4.87513006e-01,  4.11349386e-01,\n",
       "       -4.96408008e-02,  1.35047004e-01, -4.62886304e-01,  2.21131653e-01,\n",
       "       -2.06025630e-01, -4.80446368e-01,  2.27054432e-01,  5.37203431e-01,\n",
       "        7.14850724e-01,  4.28999841e-01, -1.69408724e-01, -3.85715440e-02,\n",
       "       -7.81208515e-01, -4.65835720e-01, -3.16604346e-01,  2.84540713e-01,\n",
       "        2.46599197e-01, -1.09444153e+00, -9.20692012e-02,  2.32232898e-01,\n",
       "       -4.98940609e-02, -1.06713906e-01,  6.85922325e-01,  2.93636888e-01,\n",
       "       -4.00487214e-01,  1.04259931e-01,  6.60525858e-01,  1.77994221e-01,\n",
       "       -6.16076350e-01,  1.71653312e-02,  6.14981592e-01,  2.41238832e-01,\n",
       "        5.37724197e-01, -2.76388466e-01,  9.18902978e-02,  5.21830991e-02,\n",
       "        1.10705867e-01, -3.68918091e-01,  2.26359032e-02, -1.88334242e-01,\n",
       "        1.46673068e-01,  3.36590737e-01,  9.67472196e-02,  4.73908514e-01,\n",
       "       -3.60923380e-01, -3.49651098e-01, -2.29173526e-01, -2.81934321e-01,\n",
       "        2.30019301e-01, -1.79785833e-01,  3.17300335e-02, -3.68815511e-01,\n",
       "        2.13862747e-01, -3.03516518e-02, -8.03306103e-02,  4.04847801e-01,\n",
       "        7.06272945e-02, -1.12624221e-01, -2.37838164e-01, -1.77090764e-02,\n",
       "        5.70379138e-01,  2.31017232e-01, -3.70245248e-01, -3.02313328e-01,\n",
       "        8.36544335e-01, -5.25447369e-01, -6.61720634e-01, -3.01689416e-01,\n",
       "        1.14540182e-01,  5.78046262e-01,  4.29253727e-01,  4.24993098e-01,\n",
       "       -3.32890928e-01, -7.22259939e-01, -4.98783022e-01, -2.23008537e+00,\n",
       "        1.46413758e-01,  3.12166959e-01, -2.88334459e-01,  3.56475681e-01,\n",
       "       -6.74875915e-01,  1.19462848e-01, -1.11690454e-01,  3.18117999e-02,\n",
       "        9.39079225e-01,  8.34068239e-01, -1.50240496e-01, -3.18276882e-02,\n",
       "       -2.24551986e-04, -3.11028868e-01,  5.83415866e-01,  2.08439574e-01,\n",
       "       -1.23741515e-01, -1.85864851e-01,  1.30598798e-01, -1.36630079e-02,\n",
       "       -1.16155706e-01, -9.58686024e-02, -5.74641645e-01, -7.95620680e-02,\n",
       "       -3.79281580e-01,  2.16405916e+00,  4.78177488e-01,  5.61071694e-01,\n",
       "       -2.42512207e-02,  2.93545395e-01, -3.69016267e-02, -2.47656614e-01,\n",
       "       -1.50931418e+00,  3.36007923e-01,  5.00589788e-01,  4.52306211e-01,\n",
       "        8.56361389e-02, -2.53103971e-01, -3.86732519e-02, -7.26214796e-03,\n",
       "        3.94423902e-01,  1.38382599e-01, -3.99124295e-01, -2.55438656e-01,\n",
       "       -3.63638192e-01, -2.57787973e-01, -6.13501012e-01, -1.18779317e-01,\n",
       "        4.23876435e-01,  2.15329856e-01,  3.41630057e-02,  3.08157384e-01,\n",
       "        3.13883096e-01, -4.51870441e-01, -4.20885146e-01, -1.04983902e+00,\n",
       "        1.08295027e-02, -2.46983394e-01,  4.22886848e-01, -1.32794574e-01,\n",
       "       -6.99821591e-01,  1.00948386e-01, -2.04359159e-01,  5.93272746e-01,\n",
       "       -6.62202910e-02, -5.47729321e-02, -7.35988170e-02,  4.01309818e-01,\n",
       "       -3.10818881e-01,  7.85432756e-03,  9.26734507e-01, -3.50131691e-02,\n",
       "       -3.42216015e-01,  8.69672671e-02,  2.46448919e-01,  7.19989017e-02,\n",
       "        3.82374138e-01,  3.31507891e-01, -3.96711856e-01,  3.07661533e-01,\n",
       "       -9.93290991e-02,  1.61217619e-02,  2.00741082e-01,  2.92676121e-01,\n",
       "       -3.45340699e-01,  3.26453149e-01,  1.96727249e-03,  2.70185739e-01,\n",
       "       -1.42847970e-01, -1.39063612e-01,  5.18492870e-02, -1.60198584e-01,\n",
       "        7.63220266e-02,  2.73926258e-01,  1.69689789e-01, -2.55603766e+00,\n",
       "        4.43483770e-01, -2.10989788e-01,  2.48665020e-01, -1.27635822e-01,\n",
       "        4.11634237e-01,  3.13110083e-01, -9.13701579e-02, -4.31734234e-01,\n",
       "       -8.11506901e-03, -1.97630376e-02,  8.43477100e-02,  3.41818094e-01,\n",
       "        8.41155872e-02,  1.16359048e-01,  4.25500870e-01,  7.25745499e-01,\n",
       "       -4.08002913e-01,  1.82976648e-01,  9.96399745e-02,  3.34324151e-01,\n",
       "        3.88556659e-01,  2.12828684e+00, -2.59152859e-01,  7.26905167e-01,\n",
       "        2.24438280e-01, -6.12608232e-02,  3.15206736e-01,  4.35843080e-01,\n",
       "        2.50578076e-02, -6.21489286e-02, -7.85823613e-02,  7.87079036e-01,\n",
       "       -4.09282416e-01,  2.04810113e-01,  2.42820099e-01, -8.95206779e-02,\n",
       "        2.60687530e-01,  3.79182175e-02, -8.96804873e-03, -3.59439671e-01,\n",
       "        8.83229300e-02, -6.41342461e-01, -4.02516127e-01,  6.63487971e-01,\n",
       "       -1.83429584e-01, -5.54243661e-02, -7.41993248e-01,  3.12549263e-01,\n",
       "        2.49037609e-01, -3.09501261e-01, -7.60171339e-02, -3.30602080e-01,\n",
       "        8.49305168e-02,  3.36896688e-01,  5.73989809e-01,  1.73160006e-04,\n",
       "       -5.25865220e-02, -2.33384341e-01, -2.06953004e-01,  2.58361071e-01,\n",
       "       -6.85355842e-01,  3.86066765e-01,  5.49751520e-01, -3.13830301e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's use the model and convert the sentence into embeddings\n",
    "\"\"\"\n",
    "\n",
    "embeddings = model.encode(\"I like Python programing\")\n",
    "print(f\"the length of the embedding vector is {len(embeddings)}\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8980]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can use embeddings to find the similarity between two sentences.\n",
    "Here we are using cosine similarity to find the similarity between two sentences.\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors.\n",
    "\n",
    "Maximum similarity is 1 and minimum similarity is -1.\n",
    "\"\"\"\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('This recent movie is so good')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7360]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not so similar sentences\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('I like Python programming')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exactly same sentences\n",
    "\n",
    "embeddings1 = model.encode('The new movie is awesome')\n",
    "embeddings2 = model.encode('The new movie is awesome')\n",
    "\n",
    "cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.8200\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.7016\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9697\n"
     ]
    }
   ],
   "source": [
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "    \"The new movie is awesome\",\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"The dog plays in the garden\",\n",
    "    \"A woman watches TV\",\n",
    "    \"The new movie is so great\",\n",
    "]\n",
    "\n",
    "# Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(\n",
    "        sentences1[i], sentences2[i], cosine_scores[i][i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man ate food. (Score: 0.8466)\n",
      "A man is eating a piece of bread. (Score: 0.8153)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A common use case of sentence embeddings is semantic search.\n",
    "\n",
    "Here we embed a list of documents and a query. Then we find the most similar documents to the query.\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "docs = [\n",
    "    \"A man ate food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "\n",
    "docs_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "\"\"\"\n",
    "Try different queries and see the results\n",
    "\"\"\"\n",
    "query = \"I am hungry\"\n",
    "#query = \"Tell me about music\"\n",
    "#query = \"What is moving?\"\n",
    "\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# You can set the number of results you want by changing the top_k parameter\n",
    "hits = semantic_search(query_embedding, docs_embeddings, top_k=2)\n",
    "\n",
    "for hit in hits[0]:\n",
    "    print(docs[hit['corpus_id']], \"(Score: %.4f)\" % hit['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look into ReRanking for better results\n",
    "https://sbert.net/examples/applications/retrieve_rerank/README.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "Why use tokens?\n",
    "\n",
    "> By breaking words into smaller parts (tokens), LLMs can better handle new or unusual words by understanding their building blocks. It also helps the model grasp the nuances of language, such as different word forms and contextual meanings.\n",
    "\n",
    "[source](https://kelvin.legal/understanding-large-language-models-words-versus-tokens/#:~:text=By%20breaking%20words%20into%20smaller,word%20forms%20and%20contextual%20meanings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split by whitespace: 20\n",
      "encoded sentence: 22\n",
      "tokens: [b'If', b' we', b' split', b' a', b' text', b' by', b' number', b' of', b' characters', b',', b' it', b' is', b' not', b' obvious', b' how', b' many', b' tokens', b' these', b' chunks', b' will', b' be', b'.']\n",
      "22\n",
      "reconstructed words: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If we split a text by number of characters, it is not obvious how many tokens these chunks will be.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "sent = \"If we split a text by number of characters, it is not obvious how many tokens these chunks will be.\"\n",
    "\n",
    "print(\"Split by whitespace: %s\"%len(sent.split()))\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoded = enc.encode(sent)\n",
    "\n",
    "print(\"encoded sentence: %s\"%len(encoded))\n",
    "\n",
    "tokens = [enc.decode_single_token_bytes(x) for x in encoded]\n",
    "print(\"tokens: %s\"%tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "\n",
    "decoded = enc.decode(encoded)\n",
    "print(\"reconstructed words: %s\"%len(decoded.split()))\n",
    "decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def split_large_text(large_text, max_tokens):\n",
    "    \"\"\"Convenience function to split a large text into chunks of max_tokens tokens.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokenized_text = enc.encode(large_text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for token in tokenized_text:\n",
    "        current_chunk.append(token)\n",
    "        current_length += 1\n",
    "\n",
    "        if current_length >= max_tokens:\n",
    "            chunks.append(enc.decode(current_chunk).rstrip(' .,;'))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(enc.decode(current_chunk).rstrip(' .,;'))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If we split a text by number of characters',\n",
       " ' it is not obvious how many tokens these chunks will',\n",
       " ' be.\\nAnd at the same time if we want',\n",
       " ' to split a text into bigger possible chunks and keep',\n",
       " ' these chunks under certain LLM tokens limit, we',\n",
       " ' cannot operate by number of characters']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\"If we split a text by number of characters, it is not obvious how many tokens these chunks will be.\n",
    "And at the same time if we want to split a text into bigger possible chunks and keep these chunks under certain LLM tokens limit, we cannot operate by number of characters.\"\"\"\n",
    "split_large_text(doc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the following context, then answer the question based on the context only\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = input(\"Enter the question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me about the new iPad'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = semantic_search(question, docs, top_k=2)\n",
    "\n",
    "[\"features are xxxxxx\", \"difference from preivous perverions xxxxx\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read the following context, then answer the question based on the context only\n",
    "\n",
    "Context:\n",
    "\n",
    "Apple released the iPad with xxxxxx\n",
    "\n",
    "It is difrfernt from preisou....\n",
    "\n",
    "\n",
    "Question:\n",
    "\n",
    "Tell me about the new iPad\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to read PDF in Python\n",
    "\n",
    "Here we are only reading the text, for images, tables, and formulas, we need to use OCR based solutions like nougat from Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Obtaining dependency information for pymupdf from https://files.pythonhosted.org/packages/10/be/c1a8afad3a3c1a10023548dc037c6b86b5ab8c234b6b8bc53a89c8d26051/PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from pymupdf)\n",
      "  Obtaining dependency information for PyMuPDFb==1.24.3 from https://files.pythonhosted.org/packages/7e/4a/27e4e2ce8f5d0ed1d1b2a1f7807f6158db1e8e547a7bf76ac462a800a4b4/PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.3-cp311-none-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"./multitoken.pdf\") # open a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for page in doc: # iterate the document pages\n",
    "    text += page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Better & Faster Large Language Models via Multi-token Prediction\\nFabian Gloeckle * 1 2 Badr Youbi Idrissi * 1 3 Baptiste Rozière 1 David Lopez-Paz + 1 Gabriel Synnaeve + 1\\nAbstract\\nLarge language models such as GPT and Llama\\nare trained with a next-token prediction loss. In\\nthis work, we suggest that training language mod-\\nels to predict multiple future tokens at once results\\nin higher sample efficiency. More specifically, at\\neach position in the training corpus, we ask the\\nmodel to predict the following n tokens using n\\nindependent output heads, operating on top of a\\nshared model trunk. Considering multi-token pre-\\ndiction as an auxiliary training task, we measure\\nimproved downstream capabilities with no over-\\nhead in training time for both code and natural\\nlanguage models. The method is increasingly use-\\nful for larger model sizes, and keeps its appeal\\nwhen training for multiple epochs. Gains are es-\\npecially pronounced on generative benchmarks\\nlike coding, where our models consistently out-\\nperform strong baselines by several percentage\\npoints. Our 13B parameter models solves 12 %\\nmore problems on HumanEval and 17 % more on\\nMBPP than comparable next-token models. Ex-\\nperiments on small algorithmic tasks demonstrate\\nthat multi-token prediction is favorable for the\\ndevelopment of induction heads and algorithmic\\nreasoning capabilities. As an additional benefit,\\nmodels trained with 4-token prediction are up to\\n3× faster at inference, even with large batch sizes.\\n1. Introduction\\nHumanity has condensed its most ingenious undertakings,\\nsurprising findings and beautiful productions into text.\\nLarge Language Models (LLMs) trained on all of these\\ncorpora are able to extract impressive amounts of world\\nknowledge, as well as basic reasoning capabilities by im-\\nplementing a simple—yet powerful—unsupervised learning\\ntask: next-token prediction. Despite the recent wave of\\nimpressive achievements (OpenAI, 2023), next-token pre-\\n*Equal contribution +Last authors 1FAIR at Meta 2CERMICS\\nEcole des Ponts ParisTech 3LISN Université Paris-Saclay. Cor-\\nrespondence to: Fabian Gloeckle <fgloeckle@meta.com>, Badr\\nYoubi Idrissi <byoubi@meta.com>.\\ndiction remains an inefficient way of acquiring language,\\nworld knowledge and reasoning capabilities. More precisely,\\nteacher forcing with next-token prediction latches on local\\npatterns and overlooks “hard” decisions. Consequently, it\\nremains a fact that state-of-the-art next-token predictors call\\nfor orders of magnitude more data than human children to\\narrive at the same level of fluency (Frank, 2023).\\nFigure 1: Overview of multi-token prediction. (Top) Dur-\\ning training, the model predicts 4 future tokens at once, by\\nmeans of a shared trunk and 4 dedicated output heads. Dur-\\ning inference, we employ only the next-token output head.\\nOptionally, the other three heads may be used to speed-up\\ninference time. (Bottom) Multi-token prediction improves\\npass@1 on the MBPP code task, significantly so as model\\nsize increases. Error bars are confidence intervals of 90%\\ncomputed with bootstrapping over dataset samples.\\n1\\narXiv:2404.19737v1  [cs.CL]  30 Apr 2024\\nBetter & Faster Large Language Models via Multi-token Prediction\\nIn this study, we argue that training LLMs to predict multiple\\ntokens at once will drive these models toward better sample\\nefficiency. As anticipated in Figure 1, multi-token prediction\\ninstructs the LLM to predict the n future tokens from each\\nposition in the training corpora, all at once and in parallel (Qi\\net al., 2020).\\nContributions\\nWhile multi-token prediction has been\\nstudied in previous literature (Qi et al., 2020), the present\\nwork offers the following contributions:\\n1. We propose a simple multi-token prediction architec-\\nture with no train time or memory overhead (Section 2).\\n2. We provide experimental evidence that this training\\nparadigm is beneficial at scale, with models up to 13B\\nparameters solving around 15% more code problems\\non average (Section 3).\\n3. Multi-token prediction enables self-speculative decod-\\ning, making models up to 3 times faster at inference\\ntime across a wide range of batch-sizes (Section 3.2).\\nWhile cost-free and simple, multi-token prediction is an ef-\\nfective modification to train stronger and faster transformer\\nmodels. We hope that our work spurs interest in novel aux-\\niliary losses for LLMs well beyond next-token prediction,\\nas to improve the performance, coherence, and reasoning\\nabilities of these fascinating models.\\n2. Method\\nStandard language modeling learns about a large text corpus\\nx1, . . . xT by implementing a next-token prediction task.\\nFormally, the learning objective is to minimize the cross-\\nentropy loss\\nL1 = −\\nX\\nt\\nlog Pθ(xt+1 | xt:1),\\n(1)\\nwhere Pθ is our large language model under training, as to\\nmaximize the probability of xt+1 as the next future token,\\ngiven the history of past tokens xt:1 = xt, . . . , x1.\\nIn this work, we generalize the above by implementing a\\nmulti-token prediction task, where at each position of the\\ntraining corpus, the model is instructed to predict n future\\ntokens at once. This translates into the cross-entropy loss\\nLn = −\\nX\\nt\\nlog Pθ(xt+n:t+1 | xt:1).\\n(2)\\nTo make matters tractable, we assume that our large lan-\\nguage model Pθ employs a shared trunk to produce a latent\\nrepresentation zt:1 of the observed context xt:1, then fed\\ninto n independent heads to predict in parallel each of the\\nn future tokens (see Figure 1). This leads to the follow-\\ning factorization of the multi-token prediction cross-entropy\\nloss:\\nLn = −\\nX\\nt\\nlog Pθ(xt+n:t+1 | zt:1) · Pθ(zt:1 | xt:1)\\n= −\\nX\\nt\\nn\\nX\\ni=1\\nlog Pθ(xt+i | zt:1) · Pθ(zt:1 | xt:1).\\nIn practice, our architecture consists of a shared transformer\\ntrunk fs producing the hidden representation zt:1 from the\\nobserved context xt:1, n independent output heads imple-\\nmented in terms of transformer layers fhi, and a shared\\nunembedding matrix fu. Therefore, to predict n future\\ntokens, we compute:\\nPθ(xt+i | xt:1) = softmax(fu(fhi(fs(xt:1)))),\\nfor i = 1, . . . n, where, in particular, Pθ(xt+1 | xt:1) is\\nour next-token prediction head. See Appendix B for other\\nvariations of multi-token prediction architectures.\\nMemory-efficient implementation\\nOne big challenge in\\ntraining multi-token predictors is reducing their GPU mem-\\nory utilization. To see why this is the case, recall that in\\ncurrent LLMs the vocabulary size V is much larger than the\\ndimension d of the latent representation—therefore, logit\\nvectors become the GPU memory usage bottleneck. Naive\\nimplementations of multi-token predictors that materialize\\nall logits and their gradients, both of shape (n, V ), severely\\nlimit the allowable batch-size and average GPU memory\\nutilization. Because of these reasons, in our architecture\\nwe propose to carefully adapt the sequence of forward and\\nbackward operations, as illustrated in Figure 2. In particular,\\nafter the forward pass through the shared trunk fs, we se-\\nquentially compute the forward and backward pass of each\\nindependent output head fi, accumulating gradients at the\\ntrunk. While this creates logits (and their gradients) for the\\noutput head fi, these are freed before continuing to the next\\noutput head fi+1, requiring the long-term storage only of the\\nd-dimensional trunk gradient ∂Ln/∂fs. In sum, we have\\nreduced the peak GPU memory utilization from O(nV + d)\\nto O(V + d), at no expense in runtime (Table S5).\\nInference\\nDuring inference time, the most basic use of the\\nproposed architecture is vanilla next-token autoregressive\\nprediction using the next-token prediction head Pθ(xt+1 |\\nxt:1), while discarding all others. However, the additional\\noutput heads can be leveraged to speed up decoding from the\\nnext-token prediction head with self-speculative decoding\\nmethods such as blockwise parallel decoding (Stern et al.,\\n2018)—a variant of speculative decoding (Leviathan et al.,\\n2023) without the need for an additional draft model—and\\nspeculative decoding with Medusa-like tree attention (Cai\\net al., 2024).\\n2\\nBetter & Faster Large Language Models via Multi-token Prediction\\nFigure 2: Order of the forward/backward in an n-token\\nprediction model with n = 2 heads. By performing the\\nforward/backward on the heads in sequential order, we avoid\\nmaterializing all unembedding layer gradients in memory\\nsimultaneously and reduce peak GPU memory usage.\\n3. Experiments on real data\\nWe demonstrate the efficacy of multi-token prediction losses\\nby seven large-scale experiments. Section 3.1 shows how\\nmulti-token prediction is increasingly useful when grow-\\ning the model size. Section 3.2 shows how the additional\\nprediction heads can speed up inference by a factor of 3×\\nusing speculative decoding. Section 3.3 demonstrates how\\nmulti-token prediction promotes learning longer-term pat-\\nterns, a fact most apparent in the extreme case of byte-level\\ntokenization. Section 3.4 shows that 4-token predictor leads\\nto strong gains with a tokenizer of size 32k. Section 3.5\\nillustrates that the benefits of multi-token prediction remain\\nfor training runs with multiple epochs. Section 3.6 show-\\ncases the rich representations promoted by pretraining with\\nmulti-token prediction losses by finetuning on the Code-\\nContests dataset (Li et al., 2022). Section 3.7 shows that\\nthe benefits of multi-token prediction carry to natural lan-\\nguage models, improving generative evaluations such as\\nsummarization, while not regressing significantly on stan-\\ndard benchmarks based on multiple choice questions and\\nnegative log-likelihoods.\\nTo allow fair comparisons between next-token predictors\\nand n-token predictors, the experiments that follow always\\ncompare models with an equal amount of parameters. That\\nis, when we add n −1 layers in future prediction heads, we\\nremove n −1 layers from the shared model trunk. Please\\nrefer to Table S14 for the model architectures and to Ta-\\nble S13 for an overview of the hyperparameters we use in\\nour experiments.\\n3.1. Benefits scale with model size\\nTo study this phenomenon, we train models of six sizes\\nin the range 300M to 13B parameters from scratch on at\\nleast 91B tokens of code. The evaluation results in Fig-\\n+4.5\\n-1.7\\n2\\n5\\n7\\n11 24 26\\nMBPP\\n+1.7\\n-0.6\\nPass@1\\n2\\n3\\n5\\n7\\n13 14\\nHuman Eval\\n+3.9\\n-5.4\\n10 21\\n27 36 54 57\\n+5.0\\n-1.0\\nPass@10\\n5\\n9\\n13\\n17 29 34\\n0.3B\\n0.6B\\n1.3B\\n3B\\n6.7B\\n13B\\n+2.2\\n-9.8\\n30 45 51\\n60 75 77\\n0.3B\\n0.6B\\n1.3B\\n3B\\n6.7B\\n13B\\n+7.5\\n-2.3\\nPass@100\\n11 17 24\\n30 52 56\\nFigure 3: Results of n-token prediction models on MBPP\\nby model size. We train models of six sizes in the range\\nor 300M to 13B total parameters on code, and evaluate\\npass@1,10,100 on the MBPP (Austin et al., 2021) and Hu-\\nmanEval (Chen et al., 2021) benchmark with 1000 samples.\\nMulti-token prediction models are worse than the baseline\\nfor small model sizes, but outperform the baseline at scale.\\nError bars are confidence intervals of 90% computed with\\nbootstrapping over dataset samples.\\nure 3 for MBPP (Austin et al., 2021) and HumanEval (Chen\\net al., 2021) show that it is possible, with the exact same\\ncomputational budget, to squeeze much more performance\\nout of large language models given a fixed dataset using\\nmulti-token prediction.\\nWe believe this usefulness only at scale to be a likely reason\\nwhy multi-token prediction has so far been largely over-\\nlooked as a promising training loss for large language model\\ntraining.\\n3.2. Faster inference\\nWe implement greedy self-speculative decoding (Stern\\net al., 2018) with heterogeneous batch sizes using xForm-\\ners (Lefaudeux et al., 2022) and measure decoding speeds\\nof our best 4-token prediction model with 7B parameters\\non completing prompts taken from a test dataset of code\\nand natural language (Table S2) not seen during training.\\nWe observe a speedup of 3.0× on code with an average of\\n2.5 accepted tokens out of 3 suggestions on code, and of\\n3\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTable 1: Multi-token prediction improves performance and unlocks efficient byte level training. We compare models\\nwith 7B parameters trained from scratch on 200B and on 314B bytes of code on the MBPP (Austin et al., 2021), HumanEval\\n(Chen et al., 2021) and APPS (Hendrycks et al., 2021) benchmarks. Multi-token prediction largely outperforms next token\\nprediction on these settings. All numbers were calculated using the estimator from Chen et al. (2021) based on 200 samples\\nper problem. The temperatures were chosen optimally (based on test scores; i.e. these are oracle temperatures) for each\\nmodel, dataset and pass@k and are reported in Table S12.\\nTraining data\\nVocabulary\\nn\\nMBPP\\nHumanEval\\nAPPS/Intro\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n313B bytes\\n(0.5 epochs)\\nbytes\\n1\\n19.3\\n42.4\\n64.7\\n18.1\\n28.2\\n47.8\\n0.1\\n0.5\\n2.4\\n8\\n32.3\\n50.0\\n69.6\\n21.8\\n34.1\\n57.9\\n1.2\\n5.7\\n14.0\\n16\\n28.6\\n47.1\\n68.0\\n20.4\\n32.7\\n54.3\\n1.0\\n5.0\\n12.9\\n32\\n23.0\\n40.7\\n60.3\\n17.2\\n30.2\\n49.7\\n0.6\\n2.8\\n8.8\\n200B tokens\\n(0.8 epochs)\\n32k tokens\\n1\\n30.0\\n53.8\\n73.7\\n22.8\\n36.4\\n62.0\\n2.8\\n7.8\\n17.4\\n2\\n30.3\\n55.1\\n76.2\\n22.2\\n38.5\\n62.6\\n2.1\\n9.0\\n21.7\\n4\\n33.8\\n55.9\\n76.9\\n24.0\\n40.1\\n66.1\\n1.6\\n7.1\\n19.9\\n6\\n31.9\\n53.9\\n73.1\\n20.6\\n38.4\\n63.9\\n3.5\\n10.8\\n22.7\\n8\\n30.7\\n52.2\\n73.4\\n20.0\\n36.6\\n59.6\\n3.5\\n10.4\\n22.1\\n1T tokens\\n(4 epochs)\\n32k tokens\\n1\\n40.7\\n65.4\\n83.4\\n31.7\\n57.6\\n83.0\\n5.4\\n17.8\\n34.1\\n4\\n43.1\\n65.9\\n83.7\\n31.6\\n57.3\\n86.2\\n4.3\\n15.6\\n33.7\\n2.7× on text. On an 8-byte prediction model, the inference\\nspeedup is 6.4× (Table S3). Pretraining with multi-token\\nprediction allows the additional heads to be much more ac-\\ncurate than a simple finetuning of a next-token prediction\\nmodel, thus allowing our models to unlock self-speculative\\ndecoding’s full potential.\\n3.3. Learning global patterns with multi-byte prediction\\nTo show that the next-token prediction task latches to local\\npatterns, we went to the extreme case of byte-level tokeniza-\\ntion by training a 7B parameter byte-level transformer on\\n314B bytes, which is equivalent to around 116B tokens.\\nThe 8-byte prediction model achieves astounding improve-\\nments compared to next-byte prediction, solving 67% more\\nproblems on MBPP pass@1 and 20% more problems on\\nHumanEval pass@1.\\nMulti-byte prediction is therefore a very promising avenue\\nto unlock efficient training of byte-level models.\\nSelf-\\nspeculative decoding can achieve speedups of 6 times for\\nthe 8-byte prediction model, which would allow to fully\\ncompensate the cost of longer byte-level sequences at infer-\\nence time and even be faster than a next-token prediction\\nmodel by nearly two times. The 8-byte prediction model\\nis a strong byte-based model, approaching the performance\\nof token-based models despite having been trained on 1.7×\\nless data.\\n3.4. Searching for the optimal n\\nTo better understand the effect of the number of predicted\\ntokens, we did comprehensive ablations on models of scale\\n7B trained on 200B tokens of code. We try n = 1, 2, 4, 6\\nand 8 in this setting. Results in table 1 show that training\\nwith 4-future tokens outperforms all the other models con-\\nsistently throughout HumanEval and MBPP for pass at 1,\\n10 and 100 metrics: +3.8%, +2.1% and +3.2% for MBPP\\nand +1.2%, +3.7% and +4.1% for HumanEval. Interestingly,\\nfor APPS/Intro, n = 6 takes the lead with +0.7%, +3.0%\\nand +5.3%. It is very likely that the optimal window size\\ndepends on input data distribution. As for the byte level\\nmodels the optimal window size is more consistent (8 bytes)\\nacross these benchmarks.\\n3.5. Training for multiple epochs\\nMulti-token training still maintains an edge on next-token\\nprediction when trained on multiple epochs of the same\\ndata.\\nThe improvements diminish but we still have a\\n+2.4% increase on pass@1 on MBPP and +3.2% increase\\non pass@100 on HumanEval, while having similar perfor-\\nmance for the rest. As for APPS/Intro, a window size of 4\\nwas already not optimal with 200B tokens of training.\\n3.6. Finetuning multi-token predictors\\nPretrained models with multi-token prediction loss also out-\\nperform next-token models for use in finetunings. We evalu-\\nate this by finetuning 7B parameter models from Section 3.3\\n4\\nBetter & Faster Large Language Models via Multi-token Prediction\\non the CodeContests dataset (Li et al., 2022). We compare\\nthe 4-token prediction model with the next-token prediction\\nbaseline, and include a setting where the 4-token prediction\\nmodel is stripped off its additional prediction heads and\\nfinetuned using the classical next-token prediction target.\\nAccording to the results in Figure 4, both ways of finetuning\\nthe 4-token prediction model outperform the next-token pre-\\ndiction model on pass@k across k. This means the models\\nare both better at understanding and solving the task and\\nat generating diverse answers. Note that CodeContests is\\nthe most challenging coding benchmark we evaluate in this\\nstudy. Next-token prediction finetuning on top of 4-token\\nprediction pretraining appears to be the best method overall,\\nin line with the classical paradigm of pretraining with auxil-\\niary tasks followed by task-specific finetuning. Please refer\\nto Appendix F for details.\\n1\\n10\\n100\\n1000\\nk\\n0.2\\n0.5\\n1.0\\n2.0\\n5.0\\n10.0\\npass@k (%)\\nn=1, n\\'=1\\nn=4, n\\'=1\\nn=4, n\\'=4\\nFigure 4: Comparison of finetuning performance on\\nCodeContests. We finetune a 4-token prediction model\\non CodeContests (Li et al., 2022) (train split) using n′-\\ntoken prediction as training loss with n′ = 4 or n′ = 1,\\nand compare to a finetuning of the next-token prediction\\nbaseline model (n = n′ = 1). For evaluation, we gen-\\nerate 1000 samples per test problem for each temperature\\nT ∈{0.5, 0.6, 0.7, 0.8, 0.9}, and compute pass@k for each\\nvalue of k and T. Shown is k 7→maxT pass_at(k, T), i.e.\\nwe grant access to a temperature oracle. We observe that\\nboth ways of finetuning the 4-token prediction model out-\\nperform the next-token prediction baseline. Intriguingly,\\nusing next-token prediction finetuning on top of the 4-token\\nprediction model appears to be the best method overall.\\n3.7. Multi-token prediction on natural language\\nTo evaluate multi-token prediction training on natural lan-\\nguage, we train models of size 7B parameters on 200B\\ntokens of natural language with a 4-token, 2-token and next-\\ntoken prediction loss, respectively. In Figure 5, we evaluate\\nthe resulting checkpoints on 6 standard NLP benchmarks.\\nOn these benchmarks, the 2-future token prediction model\\nperforms on par with the next-token prediction baseline\\n5000\\n10000\\n15000\\n20000\\n25000\\nTraining step\\n35.0\\n37.5\\n40.0\\n42.5\\n45.0\\n47.5\\n50.0\\n52.5\\nAverage accuracy\\nn\\n1\\n2\\n4\\nFigure 5: Multi-token training with 7B models doesn’t\\nimprove performance on choice tasks. This figure shows\\nthe evolution of average accuracy of 6 standard NLP bench-\\nmarks.\\nDetailed results in Appendix G for 7B models\\ntrained on 200B tokens of language data. The 2 future\\ntoken model has the same performance as the baseline and\\nthe 4 future token model regresses a bit. Larger model sizes\\nmight be necessary to see improvements on these tasks.\\nthroughout training. The 4-future token prediction model\\nsuffers a performance degradation. Detailed numbers are\\nreported in Appendix G.\\nHowever, we do not believe that multiple-choice and\\nlikelihood-based benchmarks are suited to effectively dis-\\ncern generative capabilities of language models. In order\\nto avoid the need for human annotations of generation qual-\\nity or language model judges—which comes with its own\\npitfalls, as pointed out by Koo et al. (2023)—we conduct\\nevaluations on summarization and natural language math-\\nematics benchmarks and compare pretrained models with\\ntraining sets sizes of 200B and 500B tokens and with next-\\ntoken and multi-token prediction losses, respectively.\\nFor summarization, we use eight benchmarks where\\nROUGE metrics (Lin, 2004) with respect to a ground-truth\\nsummary allow automatic evaluation of generated texts. We\\nfinetune each pretrained model on each benchmark’s train-\\ning dataset for three epochs and select the checkpoint with\\nthe highest ROUGE-L F1 score on the validation dataset.\\nFigure 6 shows that multi-token prediction models with both\\nn = 2 and n = 4 improve over the next-token baseline in\\nROUGE-L F1 scores for both training dataset sizes, with\\nthe performance gap shrinking with larger dataset size. All\\nmetrics can be found in Appendix H.\\nFor natural language mathematics, we evaluate the pre-\\ntrained models in 8-shot mode on the GSM8K benchmark\\n(Cobbe et al., 2021) and measure accuracy of the final an-\\nswer produced after a chain-of-thought elicited by the few-\\nshot examples. We evaluate pass@k metrics to quantify\\ndiversity and correctness of answers like in code evaluations\\n5\\nBetter & Faster Large Language Models via Multi-token Prediction\\n200\\n500\\nTraining tokens (B)\\n25.0\\n25.5\\n26.0\\n26.5\\n27.0\\n27.5\\nAvg. ROUGE-L F1\\nn=1\\nn=2\\nn=4\\nFigure 6: Performance on abstractive text summariza-\\ntion. Average ROUGE-L (longest common subsequence\\noverlap) F1 score for 7B models trained on 200B and 500B\\ntokens of natural language on eight summarization bench-\\nmarks. We finetune the respective models on each task’s\\ntraining data separately for three epochs and select the check-\\npoints with highest ROUGE-L F1 validation score. Both\\nn = 2 and n = 4 multi-token prediction models have an\\nadvantage over next-token prediction models. Individual\\nscores per dataset and more details can be found in Ap-\\npendix H.\\nand use sampling temperatures between 0.2 and 1.4. The\\nresults are depicted in Figure S13 in Appendix I. For 200B\\ntraining tokens, the n = 2 model clearly outperforms the\\nnext-token prediction baseline, while the pattern reverses\\nafter 500B tokens and n = 4 is worse throughout.\\n4. Ablations on synthetic data\\nWhat drives the improvements in downstream performance\\nof multi-token prediction models on all of the tasks we have\\nconsidered? By conducting toy experiments on controlled\\ntraining datasets and evaluation tasks, we demonstrate that\\nmulti-token prediction leads to qualitative changes in model\\ncapabilities and generalization behaviors. In particular,\\nSection 4.1 shows that for small model sizes, induction\\ncapability—as discussed by Olsson et al. (2022)—either\\nonly forms when using multi-token prediction as training\\nloss, or it is vastly improved by it. Moreover, Section 4.2\\nshows that multi-token prediction improves generalization\\non an arithmetic task, even more so than tripling model size.\\n4.1. Induction capability\\nInduction describes a simple pattern of reasoning that com-\\npletes partial patterns by their most recent continuation (Ols-\\nson et al., 2022). In other words, if a sentence contains “AB”\\nand later mentions “A”, induction is the prediction that the\\ncontinuation is “B”. We design a setup to measure induction\\n1\\n3\\n10\\n30\\n100\\n300\\n1000\\nParameters (M)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nInduction success\\nn=1 (baseline)\\nn=2 (ours)\\nFigure 7: Induction capability of n-token prediction mod-\\nels. Shown is accuracy on the second token of two token\\nnames that have already been mentioned previously. Shown\\nare numbers for models trained with a next-token and a\\n2-token prediction loss, respectively, with two independent\\nruns each. The lines denote per-loss averages. For small\\nmodel sizes, next-token prediction models learn practically\\nno or significantly worse induction capability than 2-token\\nprediction models, with their disadvantage disappearing at\\nthe size of 100M nonembedding parameters.\\ncapability in a controlled way. Training small models of\\nsizes 1M to 1B nonembedding parameters on a dataset of\\nchildren stories, we measure induction capability by means\\nof an adapted test set: in 100 stories from the original test\\nsplit, we replace the character names by randomly generated\\nnames that consist of two tokens with the tokenizer we em-\\nploy. Predicting the first of these two tokens is linked to the\\nsemantics of the preceding text, while predicting the second\\ntoken of each name’s occurrence after it has been mentioned\\nat least once can be seen as a pure induction task. In our\\nexperiments, we train for up to 90 epochs and perform early\\nstopping with respect to the test metric (i.e. we allow an\\nepoch oracle). Figure 7 reports induction capability as mea-\\nsured by accuracy on the names’ second tokens in relation\\nto model size for two runs with different seeds.\\nWe find that 2-token prediction loss leads to a vastly im-\\nproved formation of induction capability for models of size\\n30M nonembedding parameters and below, with their advan-\\ntage disappearing for sizes of 100M nonembedding parame-\\nters and above.1 We interpret this finding as follows: multi-\\ntoken prediction losses help models to learn transferring\\ninformation across sequence positions, which lends itself\\nto the formation of induction heads and other in-context\\nlearning mechanisms. However, once induction capability\\nhas been formed, these learned features transform induction\\n1Note that a perfect score is not reachable in this benchmark\\nas some of the tokens in the names in the evaluation dataset never\\nappear in the training data, and in our architecture, embedding and\\nunembedding parameters are not linked.\\n6\\nBetter & Faster Large Language Models via Multi-token Prediction\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n10\\n# operations\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nin-domain\\nout-of-domain\\nn=1\\nn=2\\nn=4\\nFigure 8: Accuracy on a polynomial arithmetic task with\\nvarying number of operations per expression. Training\\nwith multi-token prediction losses increases accuracy across\\ntask difficulties. In particular, it also significantly improves\\nout-of-domain generalization performance, albeit at a low\\nabsolute level. Tripling the model size, on the other hand,\\nhas a considerably smaller effect than replacing next-token\\nprediction with multi-token prediction loss (Figure S16).\\nShown are two independent runs per configuration with\\n100M parameter models.\\ninto a task that can be solved locally at the current token and\\nlearned with next-token prediction alone. From this point\\non, multi-token prediction actually hurts on this restricted\\nbenchmark—but we surmise that there are higher forms\\nof in-context reasoning to which it further contributes, as\\nevidenced by the results in Section 3.1. In Figure S14, we\\nprovide evidence for this explanation: replacing the chil-\\ndren stories dataset by a higher-quality 9:1 mix of a books\\ndataset with the children stories, we enforce the formation\\nof induction capability early in training by means of the\\ndataset alone. By consequence, except for the two smallest\\nmodel sizes, the advantage of multi-token prediction on the\\ntask disappears: feature learning of induction features has\\nconverted the task into a pure next-token prediction task.\\n4.2. Algorithmic reasoning\\nAlgorithmic reasoning tasks allow to measure more involved\\nforms of in-context reasoning than induction alone. We train\\nand evaluate models on a task on polynomial arithmetic in\\nthe ring F7[X]/(X5) with unary negation, addition, mul-\\ntiplication and composition of polynomials as operations.\\nThe coefficients of the operands and the operators are sam-\\npled uniformly. The task is to return the coefficients of the\\npolynomials corresponding to the resulting expressions. The\\nnumber m of operations contained in the expressions is se-\\nlected uniformly from the range from 1 to 5 at training time,\\nand can be used to adjust the difficulty of both in-domain\\n(m ≤5) and out-of-domain (m > 5) generalization evalua-\\ntions. The evaluations are conducted with greedy sampling\\non a fixed test set of 2000 samples per number of operations.\\nWe train models of two small sizes with 30M and 100M\\nnonembedding parameters, respectively. This simulates the\\nconditions of large language models trained on massive text\\ncorpora which are likewise under-parameterized and unable\\nto memorize their entire training datasets.\\nMulti-token prediction improves algorithmic reasoning ca-\\npabilities as measured by this task across task difficulties\\n(Figure 8). In particular, it leads to impressive gains in\\nout-of-distribution generalization, despite the low absolute\\nnumbers. Increasing the model size from 30M to 100M\\nparameters, on the other hand, does not improve evalua-\\ntion accuracy as much as replacing next-token prediction by\\nmulti-token prediction does (Figure S16). In Appendix K,\\nwe furthermore show that multi-token prediction models\\nretain their advantage over next-token prediction models\\non this task when trained and evaluated with pause tokens\\n(Goyal et al., 2023).\\n5. Why does it work? Some speculation\\nWhy does multi-token prediction afford superior perfor-\\nmance on coding evaluation benchmarks, and on small al-\\ngorithmic reasoning tasks? Our intuition, developed in this\\nsection, is that multi-token prediction mitigates the distri-\\nbutional discrepancy between training-time teacher forc-\\ning and inference-time autoregressive generation. We sup-\\nport this view with an illustrative argument on the implicit\\nweights multi-token prediction assigns to tokens depending\\non their relevance for the continuation of the text, as well as\\nwith an information-theoretic decomposition of multi-token\\nprediction loss.\\n5.1. Lookahead reinforces choice points\\nNot all token decisions are equally important for gener-\\nating useful texts from language models (Bachmann and\\nNagarajan, 2024; Lin et al., 2024). While some tokens allow\\nstylistic variations that do not constrain the remainder of\\nthe text, others represent choice points that are linked with\\nhigher-level semantic properties of the text and may decide\\nwhether an answer is perceived as useful or derailing.\\nMulti-token prediction implicitly assigns weights to training\\ntokens depending on how closely they are correlated with\\ntheir successors. As an illustrative example, consider the\\nsequence depicted in Figure 9 where one transition is a\\nhard-to-predict choice point while the other transitions are\\nconsidered “inconsequential”. Inconsequential transitions\\nfollowing a choice point are likewise hard to predict in\\nadvance. By marking and counting loss terms, we find that\\n7\\nBetter & Faster Large Language Models via Multi-token Prediction\\nFigure 9: Multi-token prediction loss assigns higher im-\\nplicit weights to consequential tokens. Shown is a se-\\nquence in which all transitions except “5 →A” are easy to\\npredict, alongside the corresponding prediction targets in\\n3-token prediction. Since the consequences of the difficult\\ntransition “5 →A” are likewise hard to predict, this transi-\\ntion receives a higher implicit weight in the overall loss via\\nits correlates “3 →A”, ..., “5 →C”.\\nn-token prediction associates a weight of n(n+1)\\n2\\nto choice\\npoints via their correlates, and a smaller weight of n to\\ninconsequential points. Please refer to Appendix L.3 for\\nmore details. Generally, we believe that the quality of text\\ngenerations depends on picking the right decisions at choice\\npoints, and that n-token prediction losses promote those.\\n5.2. Information-theoretic argument\\nLanguage models are typically trained by teacher-forcing,\\nwhere the model receives the ground truth for each future\\ntoken during training. However, during test time generation\\nis unguided and autoregressive, whereby errors accumulate.\\nTeacher-forcing, we argue, encourages models to focus on\\npredicting well in the very short term, at the potential ex-\\npense of ignoring longer-term dependencies in the overall\\nstructure of the generated sequence.\\nTo illustrate the impact of multi-token prediction, consider\\nthe following information-theoretic argument. Here, X\\ndenotes the next future token, and Y the second-next future\\ntoken. The production of both of these tokens is conditioned\\non some observed, input context C, that we omit from our\\nequations for simplicity. When placed before token X,\\nvanilla next-token prediction concerns the quantity H(X),\\nwhile multi-token prediction with n = 2 aims at H(X) +\\nH(Y ). We decompose these two quantities as:\\nH(X) = H(X | Y ) + I(X; Y ),\\nH(X) + H(Y ) = H(X | Y ) + 2I(X; Y ) + H(Y | X).\\nBy discarding the term H(Y | X)—which appears again\\nwhen predicting at the following position—we observe that\\n2-token prediction increases the importance of I(X; Y ) by\\na factor of 2. So, multi-token predictors are more accurate at\\npredicting tokens X that are of relevance for the remainder\\nof the text to come. In Appendix L.2, we give a relative ver-\\nsion of the above equations that shows the increased weight\\nof relative mutual information in a loss decomposition of\\n2-token prediction loss.\\n6. Related work\\nLanguage modeling losses\\nDong et al. (2019) and Tay\\net al. (2022) train on a mixture of denoising tasks with dif-\\nferent attention masks (full, causal and prefix attention) to\\nbridge the performance gap with next token pretraining on\\ngenerative tasks. Tay et al. (2022) uses the span corrup-\\ntion objective, which replaces spans of tokens with special\\ntokens for the encoder and the decoder then predicts the con-\\ntents of those spans. Unlike UniLM, this allows full causal\\ntraining with teacher forcing. Similarly, Yang et al. (2019)\\ntrain on permuted sequences, while conserving the original\\npositional embeddings, effectively training the model to pre-\\ndict various parts of the sequence given a mix of past and\\nfuture information. This permuted language modeling is\\nthe closest task to ours since it allows predicting beyond the\\nnext token. However all of these language modeling tasks\\ntrain on a small percentage of the input text: on average\\nonly 15% of the tokens are backwarded through. For Dong\\net al. (2019), where the masking is done in BERT style, it\\nis hard to mask more than 15% since it destroys too much\\ninformation. For Tay et al. (2022), it is technically possible\\nto have a larger proportion but in practice, the settings used\\nhave between 15% and 25% of masked tokens. (Yang et al.,\\n2019) also makes it possible to train on the whole sequence\\nsince it is only permuted, and no information is lost. Yet,\\nin practice, since the completely random permutation is\\nvery hard to reconstruct, only 15% are predicted for training\\nstability reasons.\\nMulti-token prediction in language modelling\\nQi et al.\\n(2020) argue that multi-token prediction encourages plan-\\nning, improves representations and prevents the overfitting\\non local patterns that can result from teacher-forced training.\\nHowever, their technical approach replicates the residual\\nstream n-fold while ours allows for compute-matched com-\\nparisons and makes the residual representations participate\\nmore directly in the auxiliary loss terms. Stern et al. (2018)\\nand Cai et al. (2024) propose model finetunings with multi-\\ntoken prediction for faster inference but do not study the\\neffects of such a loss during pretraining. Pal et al. (2023) use\\nprobing methods to show that next-token prediction models\\nare able to predict additional consecutive tokens to a certain\\nextent, but less so than our models which are specifically\\ntrained for this task. Jianyu Zhang (2024) observe improve-\\nments in language modelling tasks with multi-label binary\\nclassification over the occurrence of vocabulary words in\\nthe future as an auxiliary learning task.\\n8\\nBetter & Faster Large Language Models via Multi-token Prediction\\nSelf-speculative decoding\\nStern et al. (2018) are, to the\\nbest of our knowledge, the first to suggest a speculative\\ndecoding scheme for faster inference. Our architecture re-\\nplaces their linear prediction heads by transformer layers,\\nbut is otherwise similar. By reorganizing the order of the for-\\nward/backward, we can use all loss terms instead of stochas-\\ntically picking one head for loss computation. Cai et al.\\n(2024) present a more elaborate self-speculative decoding\\nscheme that uses the top-k predictions of each head instead\\nof the best one only. It can be used with the multi-token\\nprediction models we train.\\nMulti-target prediction\\nMulti-task learning is the\\nparadigm of training neural networks jointly on several tasks\\nto improve performance on the tasks of interest (Caruana,\\n1997). Learning with such auxiliary tasks allows models to\\nexploit dependencies between target variables and can even\\nbe preferable in the case of independent targets (Waegeman\\net al., 2019). While more specifically tailored architectures\\nfor multi-target prediction are conceivable (Spyromitros-\\nXioufis et al., 2016; Read et al., 2021), modern deep learn-\\ning approaches usually rely on large shared model trunks\\nwith separate prediction heads for the respective tasks (Caru-\\nana, 1997; Silver et al., 2016; Lample et al., 2022) like we\\ndo. Multi-target prediction has been shown to be a suc-\\ncessful strategy in various domains, e.g. for learning time\\nseries prediction with more distant time steps in the future\\nas auxiliary targets (Vapnik and Vashist, 2009) or for learn-\\ning from videos with several future frames (Mathieu et al.,\\n2016; Srivastava et al., 2016) or representations of future\\nframes (Vondrick et al., 2016) as auxiliary targets.\\n7. Conclusion\\nWe have proposed multi-token prediction as an improvement\\nover next-token prediction in training language models for\\ngenerative or reasoning tasks. Our experiments (up to 7B pa-\\nrameters and 1T tokens) show that this is increasingly useful\\nfor larger models and in particular show strong improve-\\nments for code tasks. We posit that our method reduces\\ndistribution mismatch between teacher-forced training and\\nautoregressive generation. When used with speculative de-\\ncoding, exact inference gets 3 times faster.\\nIn future work we would like to better understand how to au-\\ntomatically choose n in multi-token prediction losses. One\\npossibility to do so is to use loss scales and loss balanc-\\ning (Défossez et al., 2022). Also, optimal vocabulary sizes\\nfor multi-token prediction are likely different from those for\\nnext-token prediction, and tuning them could lead to better\\nresults, as well as improved trade-offs between compressed\\nsequence length and compute-per-byte expenses. Finally,\\nwe would like to develop improved auxiliary prediction\\nlosses that operate in embedding spaces (LeCun, 2022).\\nImpact statement\\nThe goal of this paper is to make language models more\\ncompute and data efficient. While this may in principle\\nreduce the ecological impact of training LLMs, we shall be\\ncareful about rebound effects. All societal advantages, as\\nwell as risks, of LLMs should be considered while using\\nthis work.\\nEnvironmental impact\\nIn aggregate, training all models reported in the paper re-\\nquired around 500K GPU hours of computation on hardware\\nof type A100-80GB and H100. Estimated total emissions\\nwere around 50 tCO2eq, 100% of which were offset by\\nMeta’s sustainability program.\\nAcknowledgements\\nWe thank Jianyu Zhang, Léon Bottou, Emmanuel Dupoux,\\nPierre-Emmanuel Mazaré, Yann LeCun, Quentin Garrido,\\nMegi Dervishi, Mathurin Videau and Timothée Darcet and\\nother FAIR PhD students and CodeGen team members for\\nhelpful discussions. We thank Jonas Gehring for his tech-\\nnical expertise and the original Llama team and xFormers\\nteam for enabling this kind of research.\\n9\\nBetter & Faster Large Language Models via Multi-token Prediction\\nReferences\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie Cai, Michael Terry, Quoc Le, et al.\\nProgram\\nsynthesis with large language models. arXiv preprint\\narXiv:2108.07732, 2021.\\nGregor Bachmann and Vaishnavh Nagarajan. The pitfalls\\nof next-token prediction, 2024.\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\\nShazeer. Scheduled sampling for sequence prediction\\nwith recurrent neural networks, 2015.\\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\\nGao, and Yejin Choi. Piqa: Reasoning about physical\\ncommonsense in natural language, 2019.\\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\\nJason D. Lee, Deming Chen, and Tri Dao. Medusa: Sim-\\nple llm inference acceleration framework with multiple\\ndecoding heads, 2024.\\nRich Caruana. Multitask learning. Machine learning, 28:\\n41–75, 1997.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-\\nrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda,\\nNicholas Joseph, Greg Brockman, et al.\\nEvaluating\\nlarge language models trained on code. arXiv preprint\\narXiv:2107.03374, 2021.\\nNakhun Chumpolsathien. Using knowledge distillation from\\nkeyword extraction to improve the informativeness of neu-\\nral cross-lingual summarization. Master’s thesis, Beijing\\nInstitute of Technology, 2020.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.\\nTraining verifiers to solve math word problems. arXiv\\npreprint arXiv:2110.14168, 2021.\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,\\nYu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen\\nHon. Unified language model pre-training for natural lan-\\nguage understanding and generation. In Proceedings of\\nthe 33rd International Conference on Neural Information\\nProcessing Systems, pages 13063–13075, 2019.\\nAlexandre Défossez, Jade Copet, Gabriel Synnaeve, and\\nYossi Adi. High fidelity neural audio compression. arXiv\\npreprint arXiv:2210.13438, 2022.\\nMoussa Kamal Eddine, Antoine J. P. Tixier, and Michalis\\nVazirgiannis.\\nBarthez:\\na skilled pretrained french\\nsequence-to-sequence model, 2021.\\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and\\nDragomir R. Radev. Multi-news: a large-scale multi-\\ndocument summarization dataset and abstractive hierar-\\nchical model, 2019.\\nMehrdad Farahani. Summarization using bert2bert model on\\nwikisummary dataset. https://github.com/m3hrdadfi/wiki-\\nsummary, 2020.\\nMehrdad Farahani, Mohammad Gharachorloo, and Moham-\\nmad Manthouri. Leveraging parsbert and pretrained mt5\\nfor persian abstractive text summarization. In 2021 26th\\nInternational Computer Conference, Computer Society\\nof Iran (CSICC). IEEE, March 2021.\\ndoi: 10.1109/\\ncsicc52343.2021.9420563.\\nURL http://dx.doi.\\norg/10.1109/CSICC52343.2021.9420563.\\nMichael C Frank. Bridging the data gap between children\\nand large language models. Trends in Cognitive Sciences,\\n2023.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\\nsander Wawer. Samsum corpus: A human-annotated\\ndialogue dataset for abstractive summarization. In Pro-\\nceedings of the 2nd Workshop on New Frontiers in\\nSummarization. Association for Computational Linguis-\\ntics, 2019. doi: 10.18653/v1/d19-5409. URL http:\\n//dx.doi.org/10.18653/v1/D19-5409.\\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna\\nMenon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think\\nbefore you speak: Training language models with pause\\ntokens, 2023.\\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas\\nMazeika, Akul Arora, Ethan Guo, Collin Burns, Samir\\nPuranik, Horace He, Dawn Song, et al. Measuring cod-\\ning challenge competence with apps.\\narXiv preprint\\narXiv:2105.09938, 2021.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\\nChoi. The curious case of neural text degeneration, 2020.\\nJianyu Zhang Leon Bottou. Multi-label classification as an\\nauxiliary loss for language modelling. personal commu-\\nnication, 2024.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettle-\\nmoyer. Triviaqa: A large scale distantly supervised chal-\\nlenge dataset for reading comprehension, 2017.\\nDiederik Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. ICLR, 2015.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\\nZae Myung Kim, and Dongyeop Kang. Benchmarking\\ncognitive biases in large language models as evaluators.\\narXiv preprint arXiv:2309.17012, 2023.\\n10\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,\\nMichael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin,\\nKenton Lee, Kristina N. Toutanova, Llion Jones, Ming-\\nWei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and\\nSlav Petrov. Natural questions: a benchmark for question\\nanswering research. Transactions of the Association of\\nComputational Linguistics, 2019.\\nGuillaume Lample, Marie-Anne Lachaux, Thibaut Lavril,\\nXavier Martinet, Amaury Hayat, Gabriel Ebner, Au-\\nrélien Rodriguez, and Timothée Lacroix. Hypertree proof\\nsearch for neural theorem proving, 2022.\\nYann LeCun. A path towards autonomous machine intelli-\\ngence version 0.9. 2, 2022-06-27. Open Review, 62(1),\\n2022.\\nBenjamin Lefaudeux, Francisco Massa, Diana Liskovich,\\nWenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,\\nJieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,\\nand Daniel Haziza.\\nxformers: A modular and hack-\\nable transformer modelling library. https://github.\\ncom/facebookresearch/xformers, 2022.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast\\ninference from transformers via speculative decoding,\\n2023.\\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\\nman, Julian Schrittwieser, Rémi Leblond, Tom Eccles,\\nJames Keeling, Felix Gimeno, Agustin Dal Lago, et al.\\nCompetition-level code generation with alphacode. Sci-\\nence, 378(6624):1092–1097, 2022.\\nChin-Yew Lin. ROUGE: A package for automatic evalu-\\nation of summaries. In Text Summarization Branches\\nOut, pages 74–81, Barcelona, Spain, July 2004. Asso-\\nciation for Computational Linguistics. URL https:\\n//aclanthology.org/W04-1013.\\nZhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong\\nShen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan\\nDuan, and Weizhu Chen. Rho-1: Not all tokens are what\\nyou need, 2024.\\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\\ndescent with warm restarts, 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization, 2019.\\nMichael Mathieu, Camille Couprie, and Yann LeCun. Deep\\nmulti-scale video prediction beyond mean square error,\\n2016.\\nRamesh Nallapati, Bowen Zhou, Cicero Nogueira dos san-\\ntos, Caglar Gulcehre, and Bing Xiang. Abstractive text\\nsummarization using sequence-to-sequence rnns and be-\\nyond, 2016.\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t\\ngive me the details, just the summary! topic-aware con-\\nvolutional neural networks for extreme summarization,\\n2018.\\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\\nAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam\\nMcCandlish, and Chris Olah.\\nIn-context learning\\nand induction heads.\\nTransformer Circuits Thread,\\n2022. https://transformer-circuits.pub/2022/in-context-\\nlearning-and-induction-heads/index.html.\\nOpenAI. Gpt-4 technical report, 2023.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.\\nWainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Ja-\\ncob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul Christiano, Jan\\nLeike, and Ryan Lowe. Training language models to\\nfollow instructions with human feedback, 2022.\\nKoyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace,\\nand David Bau. Future lens: Anticipating subsequent\\ntokens from a single hidden state, 2023.\\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\\nProphetnet: Predicting future n-gram for sequence-to-\\nsequence pre-training, 2020.\\nJesse Read, Bernhard Pfahringer, Geoffrey Holmes, and\\nEibe Frank. Classifier chains: A review and perspectives.\\nJournal of Artificial Intelligence Research, 70:683–718,\\n2021.\\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S\\nGordon. Choice of plausible alternatives: An evaluation\\nof commonsense causal reasoning. In 2011 AAAI Spring\\nSymposium Series, 2011.\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras,\\nand Yejin Choi. Socialiqa: Commonsense reasoning\\nabout social interactions, 2019.\\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez,\\nLaurent Sifre, George Van Den Driessche, Julian Schrit-\\ntwieser, Ioannis Antonoglou, Veda Panneershelvam,\\nMarc Lanctot, et al.\\nMastering the game of go with\\ndeep neural networks and tree search. nature, 529(7587):\\n484–489, 2016.\\n11\\nBetter & Faster Large Language Models via Multi-token Prediction\\nAaditya K Singh, Stephanie CY Chan, Ted Moskovitz, Erin\\nGrant, Andrew M Saxe, and Felix Hill. The transient\\nnature of emergent in-context learning in transformers.\\narXiv preprint arXiv:2311.08360, 2023.\\nEleftherios Spyromitros-Xioufis, Grigorios Tsoumakas,\\nWilliam Groves, and Ioannis Vlahavas. Multi-target re-\\ngression via input space expansion: treating targets as\\ninputs. Machine Learning, 104:55–98, 2016.\\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhut-\\ndinov. Unsupervised learning of video representations\\nusing lstms, 2016.\\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Block-\\nwise parallel decoding for deep autoregressive models,\\n2018.\\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Sia-\\nmak Shakeri, Dara Bahri, Tal Schuster, et al.\\nUl2:\\nUnifying language learning paradigms. arXiv preprint\\narXiv:2205.05131, 2022.\\nVladimir Vapnik and Akshay Vashist.\\nA new learning\\nparadigm: Learning using privileged information. Neural\\nnetworks, 22(5-6):544–557, 2009.\\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\\nAnticipating visual representations from unlabeled video,\\n2016.\\nWillem Waegeman, Krzysztof Dembczy´\\nnski, and Eyke\\nHüllermeier. Multi-target prediction: a unifying view\\non problems and methods. Data Mining and Knowledge\\nDiscovery, 33:293–324, 2019.\\nVikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick\\nand (not so) dirty: Unsupervised selection of justifica-\\ntion sentences for multi-hop question answering. arXiv\\npreprint arXiv:1911.07176, 2019.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\\nRuss R Salakhutdinov, and Quoc V Le. Xlnet: Gen-\\neralized autoregressive pretraining for language under-\\nstanding. In Advances in neural information processing\\nsystems, pages 5753–5763, 2019.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\\nand Yejin Choi. Hellaswag: Can a machine really finish\\nyour sentence?, 2019.\\n12\\nBetter & Faster Large Language Models via Multi-token Prediction\\nA. Additional results on self-speculative decoding\\n1\\n8\\n16\\n24\\n32\\n40\\nBatch size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nThroughput (relative)\\nk=1\\nk=2\\nk=3\\nk=4\\n1\\n8\\n16\\n24\\n32\\n40\\nBatch size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nLatency (relative)\\nk=1\\nk=2\\nk=3\\nk=4\\nFigure S10: Decoding speeds and latencies with self-speculative decoding relative to standard autoregressive decoding.\\nWe use k heads of a 4-token prediction model and evaluate decoding speeds of a code model as explained in Table S2. All\\nnumbers are relative to the autoregressive (k = 1) baseline with the same batch size.\\nTable S2: Relative speedups with self-speculative decoding. For wikipedia and books we prompt a 7B parameter model\\ntrained on 500B tokens, and for code we prompt a 7B parameter model trained on 1T tokens of code on 4200 sequences of\\n512 tokens from a test dataset not seen during training, and generate completions consisting of 512 tokens using greedy\\nself-speculative decoding (Stern et al., 2018) using the indicated number of heads from a 4-token prediction model. Note\\nthat the maximal speedup that can be obtained with self-speculative decoding using k heads is k. The last column shows the\\naverage number of tokens retrieved from a forward containing this sequence (both verification and prediction). The speedup\\nwas evaluated at the maximal batch size of 42, but is constant across batch sizes (Figure S10).\\nWikipedia\\nBooks\\nCode\\n# Heads used\\nRel. speedup\\nTokens / forward\\nRel. speedup\\nTokens / forward\\nRel. speedup\\nTokens / forward\\n1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n2\\n1.79\\n1.88\\n1.77\\n1.87\\n1.85\\n1.94\\n3\\n2.35\\n2.57\\n2.32\\n2.56\\n2.54\\n2.78\\n4\\n2.74\\n3.12\\n2.67\\n3.09\\n3.05\\n3.50\\nTable S3: Relative speedups with self-speculative decoding with byte-level models on code. We prompt the 7B parameter\\nmodels from Section 3.3 on 4096 sequences of 1024 bytes of code not seen during training, and generate completions\\nconsisting of 1024 bytes using greedy self-speculative decoding (Stern et al., 2018) as in Table S2. The speedup was\\nevaluated at a batch size of 16.\\nn = 8\\nn = 16\\nn = 32\\n# Heads used\\nRel. speedup\\nTokens / forward\\nRel. speedup\\nTokens / forward\\nRel. speedup\\ntokens / forward\\n1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n2\\n1.94\\n1.98\\n1.94\\n1.98\\n1.93\\n1.97\\n4\\n3.67\\n3.84\\n3.63\\n3.81\\n3.62\\n3.80\\n8\\n6.39\\n7.04\\n6.25\\n6.92\\n6.22\\n6.89\\n12\\n−\\n−\\n8.07\\n9.36\\n8.01\\n9.30\\n16\\n−\\n−\\n9.24\\n11.20\\n9.15\\n11.15\\n20\\n−\\n−\\n−\\n−\\n9.83\\n12.61\\n24\\n−\\n−\\n−\\n−\\n10.34\\n13.67\\n28\\n−\\n−\\n−\\n−\\n10.55\\n14.58\\n32\\n−\\n−\\n−\\n−\\n10.84\\n15.35\\n13\\nBetter & Faster Large Language Models via Multi-token Prediction\\nB. Alternative architectures\\nTable S4: Alternative architectures improve on baseline but not as consistently. Alternative architectures for multi-token\\nprediction are worth exploring to improve efficiency. Here we tried Anticausal, causal and linear and showed no significant\\nimprovement with respect to Parallel architecture.\\nMBPP\\nHumanEval\\nAPPS/Intro\\nn\\nHead type\\nArchitecture\\n+Layers\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n1\\ntransformer\\nparallel\\n0\\n30.0\\n53.8\\n73.7\\n22.8\\n36.4\\n62.0\\n2.8\\n7.8\\n17.4\\n4\\nlinear\\nparallel\\n0\\n33.6\\n55.0\\n76.2\\n21.9\\n38.5\\n63.7\\n3.1\\n10.1\\n23.0\\ntransformer\\nanticausal\\n0\\n30.8\\n54.8\\n75.3\\n20.9\\n38.4\\n64.5\\n2.0\\n8.7\\n21.6\\ncausal\\n0\\n31.9\\n54.9\\n74.9\\n20.9\\n38.1\\n67.3\\n4.0\\n11.6\\n22.8\\nparallel\\n0\\n33.8\\n55.9\\n76.9\\n24.0\\n40.1\\n66.1\\n1.6\\n7.1\\n19.9\\n3\\n33.3\\n55.7\\n77.3\\n22.4\\n39.4\\n66.7\\n2.6\\n9.5\\n22.1\\nThe architecture described in Section 2 is not the only sensible option, but proved technically viable and well-performing in\\nour experiments. We describe and compare alternative architectures in this section.\\nReplicated unembeddings\\nReplicating the unembedding matrix n times is a simple method for implementing multi-token\\nprediction architectures. However, it requires matrices with shapes (d, nV ) in the notation of Section 2, which is prohibitive\\nfor large-scale trainings.\\nLinear heads\\nApart from using a single transformer layer for the heads Hi, other architectures are conceivable. We\\nexperimented with a single linear layer without any nonlinearity as heads, amounting to linear probing of the model’s\\nresidual representation z. Architectures with more than one layer per head are also possible, but we did not pursue this\\ndirection further.\\nCausal and anticausal variant\\nInstead of making the prediction heads Pi(xt+i | zt:1) architecturally independent of each\\nother, we can also allow them to rely on other heads’ (pre-unembedding) outputs. In a causal variant, later prediction heads\\nare applied on top of the previous ones, i.e. the i-th prediction head Pi is given by\\nPθ(xt+i|·) = softmax ◦fu ◦fhi ◦fhi−1 · · · ◦fh1 ◦fs.\\nIn another anticausal variant, the network starts by predicting the most distant tokens before gradually refining up to the\\nfollowing token:\\nPθ(xt+i|·) = softmax ◦fu ◦fhi ◦fhi+1 · · · ◦fhn ◦fs.\\nThese architectures likewise allow a sequential forward/backward order as the parallel architecture from Section 2. This is\\ndescribed in Figure S11.\\n14\\nBetter & Faster Large Language Models via Multi-token Prediction\\nInput\\nTrunk\\nHead 1\\nLoss 1\\nHead 2\\nLoss 2\\n1\\n10\\n2\\n9\\n3\\n6\\n8\\n5\\n7\\n4\\nFigure S11: Order of the forward/backward in a causal n-token prediction model with n = 2 heads. Like in the\\nforward/backward depicted for parallel prediction heads in Figure 2, we avoid materializing all unembedding layer gradients\\nin memory simultaneously and reduce peak GPU memory usage significantly. The iteration over the heads starts with the\\none furthest to the trunk. At each head, a gradient from the succeeding prediction heads and from the head’s own loss are\\naccumulated for both the head’s output and its weights.\\nC. Training speeds\\nTable S5: Training time relative to next-token prediction training. The slight overhead when using multi-token prediction\\nhere is explained by a suboptimal use of Fully Sharded Data Parallel. In our implementation, when doing separate backward\\npasses for each head, we lose the overlap of layer weight communication and computation, therefore it incurs a very slight\\noverhead that can be removed if reimplemented correctly.\\nModel\\nn=1\\nn=2\\nn=4\\n0.3B\\n1.00\\n1.07\\n1.22\\n0.6B\\n1.00\\n1.05\\n1.13\\n1.3B\\n1.00\\n1.04\\n1.12\\n3B\\n1.00\\n1.02\\n1.07\\n6.7B\\n1.00\\n1.02\\n1.07\\n13B\\n1.00\\n1.04\\n1.09\\nD. Finetuning\\nTable S6: Finetuning LLama 2 with multi-token prediction does not significantly improve performance. We tried to\\nfinetune LLama 2 with 4-token prediction but this did not yield significant improvements compared to the baseline. We\\nsuppose that this new loss changes the initialization too brutally and never really recovers. We still some improvements for\\nexample on MBPP Pass@1. All runs use 200B tokens of code.\\nMBPP\\nHumanEval\\nAPPS/Intro\\nn\\nHead type\\n+Layers\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n1\\ntransformer\\n0\\n39.6\\n65.1\\n82.4\\n31.4\\n57.7\\n84.7\\n10.0\\n21.6\\n36.7\\n4\\nlinear\\n0\\n39.3\\n63.7\\n81.3\\n29.0\\n53.4\\n82.2\\n6.9\\n20.0\\n34.0\\ntransformer\\n0\\n38.3\\n62.2\\n80.1\\n27.9\\n53.6\\n82.4\\n5.8\\n18.2\\n34.3\\n3\\n42.5\\n64.4\\n81.3\\n28.7\\n56.9\\n82.4\\n7.8\\n21.2\\n37.3\\n15\\nBetter & Faster Large Language Models via Multi-token Prediction\\nE. Additional results on model scaling behavior\\nTable S7: Scaling model size Full results of scaling model size with n=1,2 and 4.\\nMBPP\\nHumanEval\\nModel Size\\nFut\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n0.3B\\n1\\n1.8\\n10.4\\n29.9\\n1.9\\n5.0\\n10.9\\n2\\n1.7\\n10.1\\n27.2\\n1.5\\n4.4\\n10.3\\n4\\n1.0\\n6.3\\n20.1\\n1.2\\n4.0\\n8.6\\n0.6B\\n1\\n4.7\\n21.0\\n45.2\\n2.9\\n8.5\\n16.7\\n2\\n4.6\\n21.0\\n44.7\\n3.2\\n8.9\\n16.2\\n4\\n3.0\\n15.6\\n38.0\\n2.7\\n7.7\\n15.5\\n1.3B\\n1\\n6.8\\n27.0\\n51.0\\n4.6\\n13.1\\n24.3\\n2\\n7.3\\n27.5\\n51.7\\n5.4\\n13.6\\n23.3\\n4\\n7.4\\n27.6\\n50.1\\n4.8\\n12.3\\n22.5\\n3B\\n1\\n11.1\\n36.4\\n60.4\\n7.2\\n17.2\\n29.8\\n2\\n11.8\\n37.2\\n60.5\\n8.0\\n18.2\\n31.2\\n4\\n12.7\\n37.6\\n61.1\\n7.2\\n18.5\\n33.3\\n6.7B\\n1\\n23.9\\n54.2\\n74.7\\n12.8\\n29.3\\n51.7\\n2\\n24.7\\n54.8\\n76.4\\n13.2\\n32.2\\n53.9\\n4\\n26.0\\n55.8\\n76.0\\n13.8\\n33.2\\n58.5\\n13B\\n1\\n26.0\\n57.1\\n77.0\\n14.1\\n33.6\\n56.0\\n2\\n30.5\\n60.5\\n79.4\\n15.2\\n36.9\\n60.0\\n4\\n30.5\\n61.0\\n79.2\\n15.8\\n38.6\\n63.5\\nF. Details on CodeContests finetuning\\nWe use the Python subset of the CodeContests (Li et al., 2022) train split with reward annotations (“correct” / “incorrect”)\\nand condition on correct solutions at evaluation time. For evaluation, we generate 1000 samples per problem from the test\\nsplit for each temperature T ∈{0.5, 0.6, 0.7, 0.8, 0.9}, and compute the unbiased estimator for pass@k from Chen et al.\\n(2021) for each value of k and T. It is possible that models that were pretrained with different losses have different respective\\noptimal temperatures for pass@k, so we compute and show k 7→maxT pass_at(k, T) in Figure 4. In other words, we grant\\npass@k access to a temperature oracle. For small values of k, pass@k measures the capability of understanding and solving\\ntasks while for large k, it additionally favors diversity in outputs. According to the results in Figure 4, multi-token prediction\\npretraining leads to finetuned models that are better on both axes.\\n16\\nBetter & Faster Large Language Models via Multi-token Prediction\\nG. Additional results on natural language benchmarks\\nWe evaluate the models from Section 3.7 on standard natural language processing benchmarks: ARC Challenge (Yadav\\net al., 2019), COPA (Roemmele et al., 2011), Hellaswag (Zellers et al., 2019), Natural Questions (Kwiatkowski et al., 2019),\\nPIQA (Bisk et al., 2019), SIQA (Sap et al., 2019) and TriviaQA (Joshi et al., 2017).\\n25\\n30\\n35\\nvalue\\narc_challenge\\n70\\n80\\ncopa\\n40\\n50\\n60\\nhellaswag\\n5\\n10\\n15\\nvalue\\nnq\\n10000\\n20000\\nglobal_step\\n65\\n70\\n75\\npiqa\\n10000\\n20000\\nglobal_step\\n42\\n44\\n46\\nsiqa\\n10000\\n20000\\nglobal_step\\n10\\n20\\n30\\n40\\nvalue\\ntqa\\nn\\n1\\n2\\n4\\nFigure S12: Multiple token training with 7B models doesn’t improve performance on choice tasks. This figure shows\\nthe evolution of average accuracy of some standard NLP benchmarks (ARC Challenge COPA Hellaswag MMLU Natural\\nQuestions PIQA SIQA and TriviaQA. For the 7B models trained on 200B tokens of language data, the 2 future token\\nmodel has the same performance as the baseline and the 4 future token model regresses a bit. Larger model sizes might be\\nnecessary to see improvements on these tasks.\\n17\\nBetter & Faster Large Language Models via Multi-token Prediction\\nH. Additional results on abstractive text summarization\\nIn this section, we report comprehensive evaluation results on summarization tasks for the 7B parameter models trained on\\n200B and 500B tokens of natural language from Section 3.7.\\nTable S8: Comprehensive evaluation on abstractive text summarization. ROUGE-n (n-gram overlap) and ROUGE-L\\n(longest common subsequence overlap) F1 scores for 7B models trained on 200B and 500B tokens of natural language,\\nrespectively. The last three columns correspond to models trained on 500B tokens, the previous three to models trained on\\n200B tokens. Shown are numbers of the n = 1 baseline and the absolute difference of n = 2 and n = 4 models trained\\non the same number of tokens. Summary-level ROUGE-L (“ROUGE-Lsum”) is reported where it differs from ROUGE-L.\\nModel checkpoints with maximal validation ROUGE-L F1 are selected separately for each model dataset and model type\\nand reported in the first row corresponding to each dataset. Boldface for numbers within 0.05 difference to the best one for\\neach dataset size separately.\\nTask\\nMetric\\nBaseline 200B\\n∆n=2\\n∆n=4\\nBaseline 500B\\n∆n=2\\n∆n=4\\nCNN/Dailymail (Nallapati et al., 2016)\\nevaluation epoch\\n2\\n2\\n2\\n2\\n2\\n2\\nROUGE-1\\n42.88\\n+0.74\\n+0.74\\n43.77\\n+0.55\\n+0.50\\nROUGE-2\\n19.56\\n+0.52\\n+0.53\\n20.34\\n+0.52\\n+0.34\\nROUGE-3\\n11.11\\n+0.39\\n+0.35\\n11.69\\n+0.36\\n+0.19\\nROUGE-L\\n29.72\\n+0.66\\n+0.49\\n30.51\\n+0.48\\n+0.37\\nROUGE-Lsum\\n40.18\\n+0.72\\n+0.68\\n41.02\\n+0.56\\n+0.52\\nMulti-News (Fabbri et al., 2019)\\nevaluation epoch\\n1\\n3\\n3\\n2\\n3\\n2\\nROUGE-1\\n44.48\\n+1.70\\n+1.72\\n45.87\\n+1.05\\n+0.69\\nROUGE-2\\n16.88\\n+0.44\\n+0.70\\n17.56\\n+0.42\\n+0.40\\nROUGE-3\\n9.63\\n-0.06\\n+0.17\\n9.91\\n+0.22\\n+0.18\\nROUGE-L\\n23.82\\n+0.17\\n+0.40\\n24.22\\n+0.20\\n+0.26\\nOrangeSum (Eddine et al., 2021)\\nevaluation epoch\\n2\\n2\\n3\\n2\\n1\\n3\\nROUGE-1\\n32.95\\n+0.41\\n+0.35\\n33.37\\n+0.32\\n+0.78\\nROUGE-2\\n13.90\\n+0.31\\n+0.36\\n14.22\\n+0.25\\n+0.53\\nROUGE-3\\n8.01\\n+0.19\\n+0.21\\n8.12\\n+0.22\\n+0.48\\nROUGE-L\\n23.62\\n+0.36\\n+0.51\\n23.91\\n+0.23\\n+0.66\\npn-summary (Farahani et al., 2021)\\nevaluation epoch\\n1\\n1\\n1\\n1\\n2\\n3\\nROUGE-1\\n1.03\\n+0.02\\n0.00\\n0.92\\n+0.09\\n+0.05\\nROUGE-2\\n0.13\\n+0.02\\n+0.03\\n0.15\\n0.00\\n0.00\\nROUGE-3\\n0.02\\n0.00\\n+0.02\\n0.02\\n0.00\\n+0.02\\nROUGE-L\\n1.02\\n+0.03\\n+0.01\\n0.91\\n+0.09\\n+0.05\\nSAMSum (Gliwa et al., 2019)\\nevaluation epoch\\n3\\n3\\n3\\n3\\n3\\n3\\nROUGE-1\\n51.39\\n+0.70\\n+0.63\\n52.54\\n-0.24\\n+0.69\\nROUGE-2\\n26.46\\n+0.76\\n+0.30\\n27.74\\n-0.20\\n+0.82\\nROUGE-3\\n16.40\\n+0.91\\n+0.28\\n17.56\\n-0.30\\n+0.71\\nROUGE-L\\n42.59\\n+0.90\\n+0.51\\n43.92\\n-0.10\\n+0.63\\nThaiSum (Chumpolsathien, 2020)\\nevaluation epoch\\n2\\n3\\n3\\n3\\n3\\n3\\nROUGE-1\\n45.08\\n+0.63\\n+1.12\\n45.48\\n+0.77\\n+0.91\\nROUGE-2\\n27.85\\n+0.30\\n+0.73\\n28.07\\n+0.74\\n+0.64\\nROUGE-3\\n15.73\\n+0.04\\n+0.43\\n15.82\\n+0.50\\n+0.30\\nROUGE-L\\n44.92\\n+0.64\\n+1.12\\n45.31\\n+0.76\\n+0.89\\nWikiSummary (Farahani, 2020)\\nevaluation epoch\\n3\\n3\\n3\\n3\\n3\\n3\\nROUGE-1\\n10.16\\n+0.67\\n-0.23\\n12.80\\n-0.17\\n-0.99\\nROUGE-2\\n4.46\\n-0.03\\n-0.09\\n6.17\\n-0.11\\n-0.69\\nROUGE-3\\n1.31\\n+0.21\\n+0.13\\n1.98\\n-0.08\\n-0.33\\nROUGE-L\\n10.11\\n+0.65\\n-0.28\\n12.69\\n-0.17\\n-0.99\\nXSum (Narayan et al., 2018)\\nevaluation epoch\\n2\\n2\\n3\\n2\\n2\\n3\\nROUGE-1\\n42.16\\n+0.71\\n+1.07\\n43.42\\n+0.78\\n+0.67\\nROUGE-2\\n19.19\\n+0.54\\n+0.55\\n20.32\\n+0.68\\n+0.34\\nROUGE-3\\n10.43\\n+0.38\\n+0.28\\n11.23\\n+0.48\\n+0.20\\nROUGE-L\\n34.03\\n+0.67\\n+0.92\\n35.18\\n+0.79\\n+0.63\\n18\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTable S9: Performance on abstractive text summarization. ROUGE-L (longest common subsequence overlap) F1 score\\nfor 7B models trained on 200B and 500B tokens of natural language. We finetune the respective models on each task’s\\ntraining data separately for a given number of epochs and select the checkpoints with maximal ROUGE-L F1 on the\\nvalidation dataset. The second and fifth column report the numbers for a next-token prediction model, while the third, fourth,\\nsixth and seventh one report the absolute improvements for 2-token and 4-token prediction models trained on the same\\namount of data, respectively. Boldface for numbers within 0.05 difference to the best one for each dataset size separately.\\nDataset\\nBaseline 200B\\n∆n=2\\n∆n=4\\nBaseline 500B\\n∆n=2\\n∆n=4\\nCNN/Dailymail\\n29.72\\n+0.66\\n+0.49\\n30.51\\n+0.48\\n+0.37\\nMulti-News\\n23.82\\n+0.17\\n+0.40\\n24.22\\n+0.20\\n+0.26\\nOrangeSum\\n23.62\\n+0.36\\n+0.51\\n23.91\\n+0.23\\n+0.66\\npn-summary\\n1.02\\n+0.03\\n+0.01\\n0.91\\n+0.09\\n+0.05\\nSAMSum\\n42.59\\n+0.90\\n+0.51\\n43.92\\n-0.10\\n+0.63\\nThaiSum\\n44.92\\n+0.64\\n+1.12\\n45.31\\n+0.76\\n+0.89\\nWikiSummary\\n10.11\\n+0.65\\n-0.28\\n12.69\\n-0.17\\n-0.99\\nXSum\\n34.03\\n+0.67\\n+0.92\\n35.18\\n+0.79\\n+0.63\\nAverage\\n26.23\\n+0.51\\n+0.46\\n27.08\\n+0.28\\n+0.31\\nTable S10: Summary statistics for abstractive text summarization evaluations. Reported are averages for ROUGE-n and\\nROUGE-L metrics across all datasets from Table S8, separately for precision, recall and F1 score. Both 2-token and 4-token\\nprediction models outperform the next-token prediction baseline. Trained on 500B tokens, 4-token prediction models appear\\nbetter at recall metrics while 2-token prediction models appear better at precision metrics. Model checkpoints are selected\\nas described in Table S8. Boldface for numbers within 0.05 difference to the best one for each dataset size separately.\\nMetric\\nAspect\\nBaseline 200B\\n∆n=2\\n∆n=4\\nBaseline 500B\\n∆n=2\\n∆n=4\\nROUGE-1\\nF1\\n33.77\\n+0.70\\n+0.68\\n34.77\\n+0.39\\n+0.41\\nprecision\\n35.76\\n+0.88\\n+0.83\\n37.03\\n+0.42\\n-0.04\\nrecall\\n34.37\\n+0.45\\n+0.45\\n35.14\\n+0.35\\n+0.68\\nROUGE-2\\nF1\\n16.06\\n+0.36\\n+0.39\\n16.82\\n+0.29\\n+0.30\\nprecision\\n16.97\\n+0.40\\n+0.43\\n17.91\\n+0.29\\n+0.03\\nrecall\\n16.34\\n+0.28\\n+0.35\\n16.99\\n+0.32\\n+0.48\\nROUGE-3\\nF1\\n9.08\\n+0.26\\n+0.23\\n9.54\\n+0.18\\n+0.22\\nprecision\\n9.59\\n+0.29\\n+0.28\\n10.17\\n+0.18\\n+0.05\\nrecall\\n9.26\\n+0.21\\n+0.20\\n9.65\\n+0.21\\n+0.35\\nROUGE-L\\nF1\\n26.23\\n+0.51\\n+0.46\\n27.08\\n+0.28\\n+0.31\\nprecision\\n27.79\\n+0.62\\n+0.55\\n28.85\\n+0.28\\n-0.09\\nrecall\\n26.71\\n+0.37\\n+0.32\\n27.40\\n+0.28\\n+0.57\\nROUGE-Lsum\\nF1\\n27.53\\n+0.52\\n+0.48\\n28.40\\n+0.29\\n+0.33\\nprecision\\n29.07\\n+0.64\\n+0.58\\n30.15\\n+0.29\\n-0.08\\nrecall\\n28.13\\n+0.35\\n+0.33\\n28.81\\n+0.29\\n+0.60\\n19\\nBetter & Faster Large Language Models via Multi-token Prediction\\nI. Additional results on mathematical reasoning in natural language\\n2\\n3\\npass@1 (%)\\n200B tokens\\n2\\n4\\n6\\n8\\n500B tokens\\nn = 1\\nn = 2\\nn = 4\\n15\\n20\\npass@10 (%)\\n20\\n30\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nTemperature\\n40\\n50\\n60\\npass@100 (%)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nTemperature\\n40\\n60\\nFigure S13: Performance on the mathematical reasoning benchmark GSM8K (Cobbe et al., 2021). We evaluate\\npretrained next-token and multi-token prediction models trained on 200B and 500B tokens of natural language in 8-shot\\nmode using nucleus sampling (Holtzman et al., 2020) with probability mass 0.95 and various sampling temperatures.\\nReported are the frequencies of the correct final answer to appear among k samples, for k = 1, 10, 100, estimated from\\n200 samples like in code generation benchmarks (Chen et al., 2021). After 200B tokens, the 2-token prediction model\\nhas a clear advantage over the next-token baseline but the order reverses after 500B tokens. The 4-token prediction model\\nis worse throughout. We interpret this similarly to the findings in Section 4.1: the follow-your-nose chains-of-thought\\nrequired for GSM8K may be difficult to learn from a limited amount of data, attesting to the data efficiency of multi-token\\nprediction training. Once the correct circuits for correct autoregressive chains-of-thought in this domain have formed,\\nhowever, multi-token prediction comes at a cost.\\n20\\nBetter & Faster Large Language Models via Multi-token Prediction\\nJ. Additional results on induction learning\\n1\\n3\\n10\\n30\\n100\\n300 1000\\nParameters (M)\\n0.850\\n0.875\\n0.900\\n0.925\\n0.950\\n0.975\\n1.000\\nInduction success\\nn=1 (baseline)\\nn=2 (ours)\\nFigure S14: Induction capability of n-token prediction models trained on higher-quality data. Shown is accuracy on the\\nsecond token of two token names that have already been mentioned previously. Training on a 9:1 mix of a books dataset and\\nthe children storiy dataset, we observe that induction capability forms significantly earlier in training (not shown here) and to\\na higher degree. We believe that this is explained both because our evaluation dataset no longer contains out-of-distribution\\ntokens (Section 4.1) and because the higher-quality data contained in the books dataset makes induction necessary earlier on\\n(especially for small models, cf. Singh et al. (2023)). In particular, by enforcing the formation of induction capability in the\\nmodel by means of the dataset – instead of the loss – the advantage of 2-token prediction models on this task disappears\\nexcept for the smallest models: feature learning converts the task into a pure next-token prediction task.\\n21\\nBetter & Faster Large Language Models via Multi-token Prediction\\nK. Additional results on algorithmic reasoning\\nWe investigate the following computation-sharing hypothesis for explaining the efficacy of multi-token prediction as training\\nloss.\\nThe prediction difficulty of different tokens in natural text varies greatly. Some tokens may be the continuations\\nof partial words that are uniquely determined from their preceding context without any effort, while others may\\nrequire to predict theorem names in difficult mathematical proofs or the correct answer to an exam question.\\nLanguage models with residual connections have been shown to refine their output token distribution with each\\nsuccessive layer, and can be trained with early exit strategies that spend variable amounts of computational\\nresources per token position. Multi-token prediction losses explicitly encourage information-sharing between\\nadjacent token positions and can thus be viewed as a method to learn allocating computational resources in\\nlanguage models more efficiently to the tokens that benefit most of it.\\nTo check the truth of this hypothesis, we augment the polynomial arithmetic task from Section 4.2 with a varying number of\\npause tokens (Goyal et al., 2023) inserted between the question and a token that denotes the beginning of the answer. Pause\\ntokens introduce additional computational resources that can be expended for computations that are expected to be useful\\nlater on in the sequence, in other words: to start thinking about the answer. According to the computation-sharing hypothesis,\\nmulti-token prediction models learn information-sharing and thus computation-sharing between token positions more easily,\\nand may be better at making use of these additional computational resources than next-token prediction models are. In\\nFigure S15, we show the evaluation results on the polynomial arithmetic task with a fixed number of pause tokens inserted\\nboth at training and evaluation time. Multi-token prediction models likewise outperform next-token prediction models\\non these task variants across task difficulties and model sizes. However, we do not see strong evidence of a widening or\\nshrinking of this gap i.e. we cannot conclude from these experiments on the veracity of the computation-sharing hypothesis.\\nIn Table S11, we report results from another experiment in the same spirit: by adding spaces and newlines to HumanEval\\nand MBPP prompts, we add “pause tokens” in a somewhat natural way. According to these results, multi-token prediction\\nmodels have a slight advantage at using this additionally provided compute, but the effect is marginal.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n10\\n# operations\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nin-domain\\nout-of-domain\\nn=1\\nn=2\\nn=4\\n(a) 5 pause tokens\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n10\\n# operations\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nin-domain\\nout-of-domain\\nn=1\\nn=2\\nn=4\\n(b) 10 pause tokens\\nFigure S15: Accuracy on a polynomial arithmetic task with varying number of operations per expression and pause\\ntokens. We train and evaluate models on the polynomial arithmetic task described in Section 4.2, modified by the addition\\nof pause tokens (Goyal et al., 2023): between the question and the equality sign that indicates the beginning of the answer,\\nwe add a constant number of pause tokens both in training and evaluation. For both a variant with five and with ten pause\\ntokens, respectively, we observe comparable improvements from using multi-token prediction to the ones obtained in the\\ncase without pause tokens (Figure 8).\\n22\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTable S11: Utilization of additional whitespace tokens in code benchmarks.\\nTask\\nWhitespace\\nn = 1\\nn = 4\\nAPPS/Intro\\nspaces + newline\\n+0.21\\n+0.34\\nAPPS/Intro\\nnewline\\n+0.79\\n+0.69\\nHumanEval\\nspaces + newline\\n-0.72\\n-0.16\\nHumanEval\\nnewline\\n-0.26\\n+0.10\\nMBPP\\nspaces + newline\\n-0.10\\n-0.06\\nMBPP\\nnewline\\n+0.03\\n-0.08\\nAverage\\n-0.01\\n+0.14\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n10\\n# operations\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nin-domain\\nout-of-domain\\n30M, n=1\\n30M, n=2\\n30M, n=4\\n100M, n=1\\n100M, n=2\\n100M, n=4\\nFigure S16: Accuracy on a polynomial arithmetic task for two model sizes. We train and evaluate models with 30M and\\n100M parameters on the polynomial arithmetic task described in Section 4.2. Tripling the model size has a smaller effect\\non performance than replacing next-token prediction loss by multi-token prediction. Shown are two independent runs per\\nconfiguration and their means, the 100M parameter models being identical to the ones in Figure 8.\\n23\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTable S12: Optimal temperatures for all numbers in table 1\\nTraining data\\nVocabulary\\nn\\nMBPP\\nHumanEval\\nAPPS/Intro\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n@1\\n@10\\n@100\\n313B bytes\\n(0.5 epochs)\\nbytes\\n1\\n0.2\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.8\\n0.8\\n0.8\\n8\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.4\\n0.4\\n0.4\\n16\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.4\\n0.4\\n0.4\\n32\\n0.1\\n0.4\\n0.8\\n0.1\\n0.4\\n0.8\\n0.1\\n0.4\\n0.4\\n200B tokens\\n(0.8 epochs)\\n32k tokens\\n1\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.1\\n0.4\\n0.8\\n2\\n0.1\\n0.8\\n0.8\\n0.2\\n0.8\\n0.8\\n0.4\\n0.4\\n0.8\\n4\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.2\\n0.8\\n0.8\\n6\\n0.1\\n0.8\\n0.8\\n0.2\\n0.8\\n0.8\\n0.4\\n0.4\\n0.8\\n8\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.2\\n0.4\\n0.8\\n1T tokens\\n(4 epochs)\\n32k tokens\\n1\\n0.1\\n0.8\\n0.8\\n0.1\\n0.8\\n0.8\\n0.1\\n0.4\\n0.8\\n4\\n0.1\\n0.8\\n0.8\\n0.2\\n0.8\\n0.8\\n0.4\\n0.8\\n0.8\\nL. Additional intuitions on multi-token prediction\\nL.1. Comparison to scheduled sampling\\nIn Section 5.2, we argued that multi-token prediction reduces the distribution mismatch between teacher-forced training and\\nautoregressive evaluation of language models. Scheduled sampling (Bengio et al., 2015) is a curriculum learning method\\nthat likewise aims to bridge this gap in sequence prediction tasks by gradually replacing more and more input tokens with\\nmodel-generated ones.\\nWhile effective in areas such as time series forecasting, scheduled sampling is, in our opinion, inapplicable to language\\nmodelling due to the discrete nature of text. Replacing ground truth input sequences by interleavings of ground truth and\\nmodel-generated tokens frequently results in ungrammatical, factually wrong or otherwise incoherent text, which should\\nbe avoided at all cost. Moreover, unlike multi-token prediction, the technique originally developed for recurrent neural\\nnetworks cannot easily be adapted for parallel training setups like the ones of transformer models.\\nL.2. Information-theoretic argument\\nWe give details on the information-theoretic terms appearing in the decomposition in Section 5.2 and derive a relative\\nversion that similarly allows to decompose multi-token prediction losses. As in Section 5.2, denote by X the next token\\nand by Y the second-next one, and omit conditioning on the preceding context C for ease of notation. In Section 5.2, we\\ndecomposed H(X) + H(Y )—the quantity of interest for 2-token prediction models—as follows:\\nH(X) + H(Y ) = H(X | Y ) + 2I(X; Y ) + H(Y | X).\\n(3)\\nLet us explain each of the terms. The entropy terms denote the uncertainty contained in the ground-truth random variables\\nX and Y . 2 The term H(Y | X) is a classical next-token entropy for the prefix (C, X). The conditional entropy H(X | Y )\\nis a more theoretical entity not modelled by causal models. It describes the uncertainty about X given the prefix C and suffix\\nY , and therefore captures the local variations of X that do not affect the continuation of the text Y . The mutual information\\nI(X; Y ) on the other hand describes the information about Y contained in X (and vice versa) and therefore captures the\\nvariations of X which constrain the continuation of the text.\\nHowever, the argument given in Section 5.2 relies on the assumption that multi-token prediction losses obey a similar\\ndecomposition as the sum of the ground-truth entropies themselves. Let us make this rigorous. Denote by p(x, y) the\\njoint distribution of X and Y , by p(x) (short for pX(x)) the marginal distribution of X and by p(y) the one of Y . Denote\\nthe densities of the model’s predictions by q(x, y), q(x) and q(y), respectively, conditional distributions by p(x | y) and\\nKullback-Leibler divergence from q to p by D(p ∥q) and cross-entropy from q to p by H(p, q).\\nDefinition L.1. The conditional cross-entropy H(pX|Y , qX|Y ) of X conditioned on Y from q to p is defined as the\\n2In particular, they do not refer to model predictions.\\n24\\nBetter & Faster Large Language Models via Multi-token Prediction\\nexpectation under y of the cross-entropy between the distributions pX and qX conditioned on y, in formulas:\\nH(pX|Y , qX|Y ) =\\nE\\ny∼pY H(pX|Y =y, qX|Y =y) =\\nE\\ny∼pY H(p(· | y), q(· | y)).\\nDefinition L.2. The relative mutual information Ip∥q(X; Y ) of X and Y from q relative to p is defined by\\nIp∥q(X; Y ) = D(p ∥qX ⊗qY ) −D(p ∥q).\\nWe have Ip∥q(X; Y ) = H(pX, qX) + H(pY , qY ) −H(p, q), Ip∥p(X; Y ) = Ip(X; Y ) reduces to standard mutual informa-\\ntion under the distribution p and Ip∥q(X; Y ) is symmetric in X and Y but can be negative.\\nWe have the following relative version of the decomposition H(X) = H(X | Y ) + I(X; Y ).\\nLemma L.3. H(pX, qX) = H(pX|Y , qX|Y ) + Ip∥q(X; Y ).\\nProof. We calculate\\nH(pX, qX) = −\\nX\\nx\\np(x) log q(x)\\n= −\\nX\\nx,y\\np(x, y) log q(x)\\n= −\\nX\\nx,y\\np(x, y) log q(x)q(y)\\np(x, y)\\np(x, y)\\nq(x, y)\\nq(x, y)\\nq(y)\\n= D(p ∥qX ⊗qY ) −D(p ∥q) −\\nX\\nx,y\\np(y)p(x | y) log q(x | y)\\n= Ip∥q(X; Y ) +\\nX\\ny\\np(y)H(pX|y, qY |y)\\n= Ip∥q(X; Y ) + H(pX|Y , qX|Y ).\\nSymmetrizing, we get the desired relative version of H(X) + H(Y ) = H(X | Y ) + 2I(X; Y ) + H(Y | X):\\nH(pX, qX) + H(pY , qY ) = H(pX|Y , qX|Y ) + 2Ip∥q(X; Y ) + H(pY |X, qY |X).\\nSetting p to be the empirical distribution of the training data, the left-hand side describes the cross-entropy loss used to\\ntrain 2-token prediction models. The right-hand side gives the decomposition into a local cross-entropy term, a mutual\\ninformation term with weight two and a shifted next-token cross-entropy term. We interpret this as follows: by adding the\\nterm H(pY , qY ) to the loss, 2-token prediction incentivizes models to precompute features which will become useful for\\npredicting Y in the next step and increases the weight of the relative mutual information term in the loss. What does relative\\nmutual information actually mean? By interpreting Kullback-Leibler divergence D(p ∥q) as the average number of bits\\nneeded in addition to send data from p with a code optimized for q instead of p, we see that minimizing\\nIp∥q(X; Y ) = D(p ∥qX ⊗qY ) −D(p ∥q)\\nmeans minimizing the average number of additional bits needed to send data from p with a code optimized for q that treats\\nX and Y as independent compared to one that does not. If this number is small, q managed to exploit the mutual information\\nof X and Y under p.\\nL.3. Lookahead reinforces choice points\\nTraining with multi-head prediction increases the importance of choice points in the loss in comparison to inconsequential\\ndecisions. To make this argument, we present a simplified model of language modelling. Consider a sequential decision task\\nand a model M that is trained in a teacher-forced way on optimal trajectories. We distinguish choice points –transitions that\\nlead to different outcomes – and inconsequential decisions which do not (Figure S17 (a) and (b)).\\n25\\nBetter & Faster Large Language Models via Multi-token Prediction☠️\\n(a)\\n(c)\\n(b)\\nFigure S17: Example of a sequential prediction task with derailing. The goal is to go from the arrow to the trophy.\\nTurning around is not allowed. Most transitions are unique, but there are two turns to be taken correctly, the consequential\\ndecisions (a) and (c). Turn (b) is an inconsequential decision: the paths join right after it. Next to transitions (a) and (b),\\nwe sketch how a 4-step prediction loss can place more emphasis on consequential transitions than inconsequential ones\\nduring teacher-forced training. Next to transition (c), we sketch how a 4-step lookahead can prevent models from taking\\nirreversible suboptimal decisions during autoregressive decoding.\\nMore formally, assume that the language model is deployed in a reinforcement learning setting like in reinforcement learning\\nfrom human feedback (Ouyang et al., 2022) (states are prompts followed by the partial sequence of tokens xt:1 generated so\\nfar, actions are single tokens xt+1 to generate, rewards are external R(xt:1)). The quantity\\nVπ(xt:1) = Ext+i∼π(xt+i−1:1),i≥1\\n\\uf8ee\\n\\uf8f0X\\ni≥0\\nR(xt+i:1)\\n\\uf8f9\\n\\uf8fb\\nis the value of the state xt:1 following the policy π, while\\nσπ(xt:1) =\\nr\\nVar\\nxt+1∼π(xt:1) [Vπ(xt+1:1)]\\nquantifies the importance of the decision xt+1 on the value thereafter. Choice points can formally be viewed as steps t for\\nwhich σπ(xt:1) is large, while inconsequential points are steps where it is low. Note that for completion models, there is no\\nexplicit reward, and our argument is merely meant to illustrate what we mean by choice points.\\nDerailing denotes a situation where autoregressive generation of trajectories from M at inference time results in bad\\noutcomes after M made a mistake on a choice point. Even if subsequently, M acts optimally given this choice, the final\\noutcome can be significantly worse than the outcome of the optimal trajectory.\\nStaying in the teacher-forced setting, we ask: What is the impact of training M with n-step prediction instead of next-\\nstep prediction on this task? Say xt →xt+1 is a choice point in an optimal trajectory with the suboptimal choice\\nbeing xt →˜\\nxt+1 (Figure S17 (a)). Assume that the trajectories preceding xt and succeeding xt+1 and ˜\\nxt+1 consist of\\ninconsequential transitions, the latter denoted by ˜\\nxt+j →˜\\nxt+j+1. We will compare the losses of a teacher-forced next-step\\nprediction model and a teacher-forced n-step prediction model on the partial trajectory (xt−n+1, . . . xt). For the next-step\\nprediction model, the predictions are (xt−n+2, . . . , xt, ˜\\nxt+1) with a single wrong prediction. The predictions of an n-step\\nprediction model at time t −n + i, i = 1, . . . , n are (xt−n+i+1, . . . , xt, ˜\\nxt+1, . . . , ˜\\nxt+i) with i wrong predictions. In other\\nwords, an n-step prediction model receives 1 + . . . + n = n(n+1)\\n2\\nloss terms pertaining to such a choice point and its\\nconsequences, while each inconsequential transition (Figure S17 (b)) is only reinforced n times as often as in a next-step\\nprediction model. In other words, choice points receive on average n+1\\n2\\ntimes more importance in the loss of n-step\\nprediction models than in next-step prediction models.\\n26\\nBetter & Faster Large Language Models via Multi-token Prediction\\nAs argued in Section 5.1, we believe that this model captures important features of training and inference with language\\nmodels: choice points are semantically important turning points in the generated texts, such as the final answer to a question\\nor a specific line of code, while inconsequential decisions can be a choice among synonyms or of variable names in code.\\nApart from this training dynamics point of view, we hypothesize that n-step prediction also allows the formation of circuits\\nthat specifically spot inconsistencies between predictions for earlier and later steps. For instance, if in an early layer of\\nthe model, it can be predicted that a decision xt →˜\\nxt+1 leads to suboptimal outcomes ˜\\nxt+n (Figure S17 (c)), subsequent\\nlayers can reduce the probability of xt →˜\\nxt+1 in the model’s next-step prediction. Such behaviors also happen in next-step\\nprediction models given enough capacity, but our experiments in Section 4.2 point to the fact that circuits of this kind are\\nformed more easily in multi-step architectures that enforce the required information ˜\\nxt+n to be available to the model when\\npredicting ˜\\nxt+1. We believe that this situation appears frequently in natural language and code modelling, for instance where\\nan initial answer to a question contradicts the results of the chain of thought brought forward with the intention to justify it.\\nIn more general terms, this situation arises whenever predicting first ˜\\nxn+i for some 1 < i ≤n and then ˜\\nxn+1 based on ˜\\nxn+i\\nis easier than predicting ˜\\nxn+1 directly. We discuss this phenomenon of factorization orders in the next section and present a\\nspecific instance of it that frequently appears in modelling natural language.\\nL.4. Factorization orders\\nCausal language modelling factorizes probabilities over text sequences xt · · · x1 classically as\\nP(xt · · · x1) =\\nt\\nY\\ni=1\\nP(xi | xi−1 · · · x1).\\nWhile moving forward in time is certainly the most natural choice of factorization order, there exist cases where it is\\nsuboptimal. In inflectional languages, for instance, agreement between related sentence parts is a frequent pattern with one\\nword directing the grammatical forms of others. Consider the German sentence\\nWie konnten auch Worte meiner durstenden Seele genügen?3\\nFriedrich Hölderlin, Fragment von Hyperion (1793)\\nwhere \"genügen\" requires a dative case object and then \"Seele\" requires the possessive pronoun \"mein\" to be in female\\nsingular dative form \"meiner\" and the participle \"durstend\" to be in female singular dative form in weak declination\\n\"durstenden\" because it follows \"meiner\". In other words, the factorization order\\nWie konnten auch Worte →genügen →Seele →meiner →durstenden?\\nis arguably an easier one for constructing the above sentence. Humans as well as language models therefore have to perform\\nthis factorization (which deviates from the causal order in which predictions take place!) within their latent activations, and\\na 4-token prediction loss makes this easier as it explicitly encourages models to have all information about the successive 4\\ntokens in its latent representations.\\n3roughly: How could words be enough for my thirsty soul?\\n27\\nBetter & Faster Large Language Models via Multi-token Prediction\\nM. Training hyperparameters\\nTable S13: Overview of all training hyperparameters used. We schedule all learning rates with a linear warmup and\\ncosine decay (Loshchilov and Hutter, 2017) to a fraction of the peak learning rate which is depicted in the last column\\n(“decay ratio”). All experiments use the Adam (Kingma and Ba, 2015) optimizer with β1 = 0.9, β2 = 0.95 and decoupled\\nL2 weight decay (Loshchilov and Hutter, 2019) coefficient 0.1. We clip gradients to a maximal Euclidean norm of 1.0 in all\\nexperiments except CodeContests finetunings, where we use 0.1 instead. Summarization finetunings correspond to three\\nepochs on all datasets except BigPatent (1 epoch). Byte-level models use the architecture with replicated unembeddings\\nfrom Appendix B.\\nModel\\nBatch size (220)\\nSteps\\nTokens (B)\\nWarmup steps\\nPeak LR\\nContext length\\nDecay ratio\\nModel scaling (Section 3.1)\\n0.3B\\n8\\n10,850\\n91.0\\n1000\\n3 × 10−4\\n4096\\n0.03\\n0.6B\\n8\\n10,850\\n91.0\\n1000\\n3 × 10−4\\n4096\\n0.03\\n1.3B\\n8\\n10,850\\n91.0\\n1000\\n3 × 10−4\\n4096\\n0.03\\n3B\\n8\\n10,850\\n91.0\\n1000\\n3 × 10−4\\n4096\\n0.03\\n7B\\n8\\n25,000\\n209.7\\n2000\\n3 × 10−4\\n4096\\n0.03\\n13B\\n8\\n25,000\\n209.7\\n1000\\n3 × 10−4\\n4096\\n0.03\\nCode models (Section 3)\\n7B 200B\\n8\\n25,000\\n209.7\\n2000\\n3 × 10−4\\n4096\\n0.03\\n7B 500B\\n7\\n68,570\\n503.3\\n2000\\n3 × 10−4\\n4096\\n0.03\\n7B 1T\\n7\\n136,240\\n1000.0\\n2000\\n3 × 10−4\\n4096\\n0.03\\nByte-level models (Section 3.3)\\n7B 314GB\\n12\\n25,000\\n314.6\\n2000\\n3 × 10−4\\n8192\\n0.03\\nLanguage models (Section 3.7)\\n7B 200B\\n8\\n25,000\\n209.7\\n2000\\n3 × 10−4\\n4096\\n0.10\\n7B 500B\\n8\\n60,000\\n503.3\\n2000\\n3 × 10−4\\n4096\\n0.10\\nInduction task (Section 4.1)\\n1M – 1B\\n0.25\\n100,000\\n26.2\\n2000\\n10−4\\n2048\\n0.03\\n1M – 1B (Appendix J)\\n0.5\\n50000\\n26.2\\n2000\\n10−4\\n2048\\n0.03\\nArithmetic task (Section 4.2)\\n30M\\n0.25\\n100,000\\n26.2\\n2000\\n10−4\\n1024\\n0.03\\n100M\\n0.25\\n100,000\\n26.2\\n2000\\n10−4\\n2048\\n0.03\\nSummarization (Section 3.7)\\nBigPatent\\n0.125\\n76,680\\n10.1\\n100\\n3 × 10−5\\n4096\\n0.03\\nCNN/Dailymail\\n0.125\\n7,140\\n0.9\\n100\\n3 × 10−5\\n4096\\n0.03\\nMulti-News\\n0.125\\n3,330\\n0.4\\n100\\n3 × 10−5\\n4096\\n0.03\\nOrangeSum\\n0.125\\n360\\n0.0\\n100\\n3 × 10−5\\n4096\\n0.03\\npn-summary\\n0.125\\n3,450\\n0.5\\n100\\n3 × 10−5\\n4096\\n0.03\\nSAMSum\\n0.125\\n60\\n0.0\\n100\\n3 × 10−5\\n4096\\n0.03\\nThaiSum\\n0.125\\n23,640\\n3.1\\n100\\n3 × 10−5\\n4096\\n0.03\\nWikiSummary\\n0.125\\n2,550\\n0.3\\n100\\n3 × 10−5\\n4096\\n0.03\\nXSum\\n0.125\\n2,760\\n0.4\\n100\\n3 × 10−5\\n4096\\n0.03\\nCodeContests (Section 3.6)\\n7B\\n0.25\\n13,000\\n3.6\\n400\\n5 × 10−5\\n4096\\n0.004\\n28\\nBetter & Faster Large Language Models via Multi-token Prediction\\nTable S14: Overview of model architectures used for scaling analyses.\\nName\\nDimension\\nLayers\\nHeads\\n1M\\n128\\n5\\n4\\n3M\\n256\\n4\\n8\\n10M\\n384\\n6\\n8\\n30M\\n512\\n10\\n8\\n100M\\n768\\n14\\n12\\n300M\\n1024\\n25\\n16\\n1B\\n1536\\n36\\n24\\n0.3B\\n1024\\n18\\n16\\n0.6B\\n1280\\n27\\n20\\n1.3B\\n2048\\n24\\n16\\n3B\\n2560\\n36\\n20\\n6.7B (“7B”)\\n4096\\n32\\n32\\n13B\\n5120\\n40\\n40\\n29\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PostgreSQL as the embedding storage\n",
    "\n",
    "When you have large amounts of documents that does not fit in your local computer, you need to store them in a database.\n",
    "\n",
    "PostgreSQL is a conventient choice for vector store because we probably already store other business data in the same database\n",
    "\n",
    "Vector embedding support is provided by a postgres extension called [pgvector](https://github.com/pgvector/pgvector)\n",
    "\n",
    "You can learn more about PGVector and working with embeddings in Postgres on this [tutorial](https://supabase.com/docs/guides/ai) provided by Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name Supabase/gte-small. Creating a new one with MEAN pooling.\n",
      "/Users/ian/Code/school/techin510/sp24/lab7/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up libraries and variables\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import fitz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from utils import split_large_text\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CHUNK_TOKEN_SIZE = 512\n",
    "con = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n",
    "model = SentenceTransformer('Supabase/gte-small')\n",
    "\n",
    "# enable the vector datatype in psycopg2\n",
    "register_vector(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create our table to store the chunked document and its embeddings\n",
    "\"\"\"\n",
    "\n",
    "docs_ddl = \"\"\"create table if not exists document_sections (\n",
    "  id bigint primary key generated always as identity,\n",
    "  content text not null,\n",
    "  embedding vector (384)\n",
    ");\"\"\"\n",
    "\n",
    "with con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(docs_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    './linkedin_report.pdf',\n",
    "    './multitoken.pdf',\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Read the two pdfs and split the text into chunks\n",
    "\"\"\"\n",
    "split_text_list = []\n",
    "for pdf_file in pdf_files:\n",
    "    doc = fitz.open(pdf_file, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in doc:  # iterate the document pages\n",
    "        text += page.get_text()\n",
    "    split_text_list.extend(split_large_text(text, CHUNK_TOKEN_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the embeddings for the chunks and insert them into the database\n",
    "\"\"\"\n",
    "\n",
    "doc_embeddings = model.encode(split_text_list)\n",
    "\n",
    "with con:\n",
    "    with con.cursor() as cur:\n",
    "        for content, embedding in zip(split_text_list, doc_embeddings):\n",
    "            cur.execute(\"insert into document_sections (content, embedding) values (%s, %s)\", (content, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the index on the embedding column to enable fast similarity search\n",
    "\n",
    "We are using the hnsw index for this purpose\n",
    "\n",
    "Learn more about the index here: https://supabase.com/docs/guides/ai/vector-indexes\n",
    "\"\"\"\n",
    "\n",
    "with con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('CREATE INDEX ON document_sections USING hnsw (embedding vector_cosine_ops)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI\n",
      "Musings on building a Generative AI\n",
      "product\n",
      "Juan Pablo Bottaro\n",
      "April 25, 2024\n",
      "Co-authors: Juan Pablo Bottaro and Karthik Ramgopal\n",
      "Over the last six months, our team here at LinkedIn has been working hard to\n",
      "develop a new AI-powered experience. We wanted to reimagine how our members\n",
      "go about their job searches and browse professional content.\n",
      "The explosion of generative AI made us pause and consider what was possible now\n",
      "that wasn’t a year ago. We tried many ideas which didn’t really click, eventually\n",
      "discovering the power of turning every feed and job posting into a springboard to:\n",
      "Get information faster, e.g. takeaways from a post or learn about the latest\n",
      "from a company.\n",
      "Connect the dots, e.g. assess your fit for a job posting.\n",
      "Receive advice, e.g. improve your profile or prepare for an interview.\n",
      "And much more…\n",
      "Was it easy to build? What went well and what didn’t? Building on top of\n",
      "generative AI wasn’t all smooth sailing, and we hit a wall in many places. We want\n",
      "to pull back the “engineering” curtain and share what came easy, where we\n",
      "struggled, and what’s coming next.\n",
      "Overview\n",
      "Let’s walk through a real-life scenario to show how the system works.\n",
      "Imagine you're scrolling through your LinkedIn feed and stumble upon an\n",
      "intriguing post about accessibility in design. Alongside the post, you're presented\n",
      "with a few starter questions to delve deeper into the topic. You're curious and click\n",
      "on, \"What are some examples of accessibility driving business value in tech\n",
      "companies?\"\n",
      "Here’s what happens in the background:\n",
      "1. Pick the right agent: This is where your journey begins. Our system takes\n",
      "your question and decides which AI agent is best equipped to handle it. In\n",
      "this case, it recognizes your interest in accessibility within tech companies\n",
      "and routes your query to an AI agent specialized in general knowledge\n",
      "seeking questions.\n",
      "2. Gather information: It’s time for some legwork. The AI agent calls a\n",
      "combination of internal APIs & Bing, searching for specific examples and\n",
      "case studies that highlight how accessibility in design has contributed to\n",
      "business value in tech. We are creating a dossier to ground our response.\n",
      "3. Craft a response: With the necessary information in hand, the agent can\n",
      "now write a response. It filters and synthesizes the data into a coherent,\n",
      "informative answer\n",
      "chimed in (product, eng, design, etc.), but we knew we needed a more\n",
      "principled approach with consistent and diverse annotators. Our internal\n",
      "linguist team built tooling and processes by which we could evaluate up to\n",
      "500 daily conversations and get metrics around: overall quality score,\n",
      "hallucination rate, Responsible AI violation, coherence, style, etc. This\n",
      "became our main signpost to understand trends, iterate on prompts &\n",
      "ensure we were ready to go live.\n",
      "Automatic evaluation is the holy grail, but still a work in progress.\n",
      "Without it, engineers are left with eye-balling results and testing on a limited\n",
      "set of examples, and having a 1+ day delay to know metrics. We are building\n",
      "model-based evaluators to estimate the above metrics & allow for much\n",
      "faster experimentation, and had some success on hallucination detection\n",
      "(but it wasn’t easy!).\n",
      "Figure 2: Evaluation steps we perform. Engineers perform fast, coarse evaluations to get directional\n",
      "metrics. Annotators give more granular feedback but have a ~1 day turnaround. Members are the final\n",
      "judges and give us scale, but some metrics can take 3+ days for a single change\n",
      "What we are working on: end-to-end automatic evaluation pipeline for faster\n",
      "iteration.\n",
      "Calling internal APIs\n",
      "LinkedIn has a lot of unique data about people, companies, skills, courses, etc.\n",
      "which are critical to building a product offering unique and differentiated value.\n",
      "LLMs, however, have not been trained with this information and hence are unable\n",
      "to use them as is for reasoning and generating responses. A standard pattern to\n",
      "work around this is to set up a Retrieval Augmented Generation (RAG) pipeline, via\n",
      "which internal APIs are called, and their responses are injected into a subsequent\n",
      "LLM prompt to provide additional context to ground the response.\n",
      "A lot of this unique data is exposed internally via RPC APIs across various\n",
      "microservices. While this is very convenient for humans to invoke\n",
      "programmatically, it is not very LLM friendly. We worked around this by wrapping\n",
      "“skills” around these APIs. Every skill has the following components:\n",
      "A human (and hence LLM) friendly description of what the API does, and\n",
      "when to use it.\n",
      "The configuration to call the RPC API (Endpoint, input schema, output\n",
      "schema etc.)\n",
      "The LLM friendly input and output schema\n",
      "Primitive typed (String/Boolean/Number) values\n",
      "JSON schema style input and\n",
      ", providing you with clear examples of how accessibility\n",
      "initiatives have driven business value for tech companies. To avoid\n",
      "generating a wall of text and make the experience more interactive, internal\n",
      "APIs are invoked to decorate the response with attachments like article\n",
      "links, or profiles of people mentioned in the post.\n",
      "You might follow up with “How do I pivot my career towards this area?”, and we’d\n",
      "repeat the process but now routing you to a career and job AI agent. With just a few\n",
      "clicks you can go deep on any topic, get actionable insights or find your next big\n",
      "opportunity.\n",
      "Most of this was made possible by the advent of large language models (LLMs), and\n",
      "we thought it’d be interesting to share the behind-the-scenes stories about the\n",
      "challenges we faced building on top of them.\n",
      "What came easy\n",
      "Overall design\n",
      "Figure 1: Simplified pipeline for handling user queries. KSA stands for “Knowledge Share Agent”\n",
      ", one of\n",
      "the dozens of agents that can handle user queries\n",
      "Some of you might’ve noticed from the explanation above that our pipeline follows\n",
      "what’s known as Retrieval Augmented Generation (RAG), which is a common\n",
      "design pattern with generative AI systems. Building the pipeline was surprisingly\n",
      "less of a headache than we anticipated. In just a few days we had the basic\n",
      "framework up and running:\n",
      "Routing: decides if the query is in scope or not, and which AI agent to\n",
      "forward it to. Examples of agents are: job assessment, company\n",
      "understanding, takeaways for posts, etc.\n",
      "Retrieval: recall-oriented step where the AI agent decides which services to\n",
      "call and how (e.g. LinkedIn People Search, Bing API, etc.).\n",
      "Generation: precision-oriented step that sieves through the noisy data\n",
      "retrieved, filters it and produces the final response.\n",
      "Tuning ‘routing’ and ‘retrieval’ felt more natural given their classification nature: we\n",
      "built dev sets and fitted them with prompt engineering and in-house models. Now,\n",
      "generation, that was a different story. It followed the 80/20 rule; getting it 80% was\n",
      "fast, but that last 20% took most of our work. When the expectation from your\n",
      "product is that 99%+ of your answers should be great, even using the most\n",
      "advanced models available still requires a lot of work and creativity to gain every 1%.\n",
      "What worked for us:\n",
      "Fixed 3-step pipeline\n",
      "\n",
      " output schema descriptions\n",
      "The business logic to map between LLM friendly schemas and actual RPC\n",
      "schemas.\n",
      "Skills like this enable the LLM to do various things relevant to our product like view\n",
      "profiles, search articles/people/jobs/companies and even query internal analytics\n",
      "systems. The same technique is also used for calling non-LinkedIn APIs like Bing\n",
      "search and news.\n",
      "Figure 3: Calling internal APIs using skills\n",
      "We write prompts that ask the LLM to decide what skill to use to solve a particular\n",
      "job (skill selection via planning), and then also output the parameters to invoke the\n",
      "skill with (function call). Since the parameters to the call have to match the input\n",
      "schema, we ask the LLM to output them in a structured manner. Most LLMs are\n",
      "trained on YAML and JSON for structured output. We picked YAML because it is\n",
      "less verbose, and hence consumes fewer tokens than JSON.\n",
      "One of the challenges we ran into was that while about ~90% of the time, the LLM\n",
      "responses contained the parameters in the right format, ~10% of the time the LLM\n",
      "would make mistakes and often output data that was invalid as per the schema\n",
      "supplied, or worse not even valid YAML. These mistakes, while being trivial for a\n",
      "human to spot, caused the code parsing them to barf. ~10% was a high enough\n",
      "number for us to not ignore trivially, and hence we set out to fix this problem.\n",
      "A standard way to fix this problem is to detect it and then re-prompt the LLM to ask\n",
      "it to correct its mistakes with some additional guidance. While this technique works,\n",
      "it adds a non-trivial amount of latency and also consumes precious GPU capacity\n",
      "due to the additional LLM call. To circumvent these limitations, we ended up\n",
      "writing an in-house defensive YAML parser.\n",
      "Through an analysis of various payloads, we determined common mistakes made by\n",
      "the LLM, and wrote code to detect and patch these appropriately before parsing. We\n",
      "also modified our prompts to inject hints around some of these common mistakes,\n",
      "to improve the accuracy of our patching. We were ultimately able to reduce\n",
      "occurrences of these errors to ~0.01%.\n",
      "What we are working on: a unified skill registry to dynamically discover and\n",
      "invoke APIs/agents packaged as LLM friendly skills across our generative AI\n",
      "products.\n",
      "Consistent quality\n",
      "The team achieved 80% of the basic experience we were aiming to provide within\n",
      "the first month and\n",
      "-Stage Innovations\n",
      "Data Analysis and Initial Results\n",
      "Below we report initial results from our pilot study with [Anonymized]’s expert screeners. This section\n",
      "will be updated in June 2024 to include the full experimental results encompassing a larger set of\n",
      "professional screeners (e.g., VCs, executives, entrepreneurs, MBAs).\n",
      "Time to Decision. First, we investigated the time spent evaluating each solution, comparing the\n",
      "differences across the three conditions. Overall, the screeners spent, on average, 1.98 minutes per solution\n",
      "(s.d. = 0.50 minutes). Figure 2a shows no meaningful differences in time spent across the experimental\n",
      "conditions.\n",
      "Figure 2a (left). Average Time Spent Evaluating Solutions By\n",
      "Experimental Condition (Seconds) Figure 2b (right). Proportion of\n",
      "Decisions Assigned a Pass Rating By Experimental Condition\n",
      "Stringency of Decisions (% Pass): Next, we compare the stringency of different experimental conditions\n",
      "by investigating the proportion of evaluation passed across the three conditions (see Figure 2b). Whereas\n",
      "70.6% of screener-solution pairs were assigned a pass rating in the control condition, 65.5% passed in\n",
      "Treatment 1, and 58.3% passed in Treatment 2. A t-test indicates that the difference between the control\n",
      "condition and Treatment 2 is statistically significant (p = 0.015).\n",
      "Alignment with AI’s Recommendation: Third, we investigate how the screening decisions aligned with\n",
      "AI’s recommendations. Whereas 55.6% of the control condition’s decisions were aligned with AI’s\n",
      "recommendations, we observe that 71.7% of Treatment 1 (p = 0.001) and 66.1% of Treatment 2’s decisions\n",
      "(p=0.040) were aligned with AI’s. This suggests that AI’s recommendations strongly influenced the\n",
      "screeners’ decisions, but there was no significant difference between the two treatment conditions.\n",
      "Figure 3b investigates differences in alignment according to the AI’s recommendation to pass or fail a\n",
      "solution. We observe that AI’s recommendations were overall more stringent (i.e., more failed solutions)\n",
      "than the human’s recommendations (i.e., the control condition). Whereas there is no difference in\n",
      "alignment across the experimental conditions for the passing decisions, we find a significant difference\n",
      "among the failed solutions. In particular, 37% in the control condition failed to pass the screening criteria,\n",
      "compared to\n"
     ]
    }
   ],
   "source": [
    "# Change the query here to see the results\n",
    "query = \"What is important in the linkedin report?\"\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "with con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute('SELECT * FROM document_sections ORDER BY embedding <-> %s LIMIT 5', (query_embedding,))\n",
    "        res = cur.fetchall()\n",
    "\n",
    "for r in res:\n",
    "    print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
